"""
Tutor Agent: Provides adaptive teaching explanations.

The Tutor offers explanations and guidance tailored to the student's
cognitive level, learning style, and current understanding.
"""
from typing import Dict, Any, List
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_core.messages import HumanMessage, AIMessage
from app.agents.state.teaching_state import TeachingState
from app.agents.state.level_prompts import get_tutor_prompt, get_level_description
from app.core.config import settings, get_model_name
from app.core.chroma import query_document_chunks


class TutorAgent:
    """
    The Tutor is responsible for:
    1. Providing explanations adapted to student level
    2. Answering questions in an appropriate style
    3. Offering hints and guidance
    4. Adjusting explanations based on student responses
    """

    def __init__(self, api_key: str = None, model: str = None):
        """Initialize the Tutor agent with DashScope LLM."""
        self.llm = ChatTongyi(
            dashscope_api_key=api_key or settings.DASHSCOPE_API_KEY,
            model_name=model or settings.DEFAULT_MODEL,
            temperature=0.7  # Higher for more varied, natural responses
        )

    async def explain_concept(
        self,
        state: TeachingState,
        topic: str = None,
        context: str = None
    ) -> str:
        """
        Generate an explanation for a concept or topic.

        Args:
            state: Current teaching state
            topic: Specific topic to explain (optional)
            context: Additional context or question (optional)

        Returns:
            Formatted explanation text with document source references
        """
        student_level = state["student_level"]
        chapter_content = state["chapter_content"]
        chapter_title = state["chapter_title"]
        learning_objectives = state.get("learning_objectives", [])
        wrong_questions = state.get("wrong_questions", [])
        document_md5 = state.get("document_md5", "")

        # RAG æ£€ç´¢ï¼šæŸ¥æ‰¾ç›¸å…³æ–‡æ¡£å†…å®¹ä½œä¸ºæ¥æº
        document_sources = ""
        if document_md5:
            try:
                from dashscope import TextEmbedding

                # ç”ŸæˆæŸ¥è¯¢å‘é‡
                query_text = topic or f"{chapter_title} {context or 'æ ¸å¿ƒæ¦‚å¿µ'}"
                response = TextEmbedding.call(
                    model='text-embedding-v2',
                    input=query_text,
                    text_type='query'
                )

                if response.status_code == 200:
                    query_embedding = response.output['embeddings'][0]['embedding']

                    # æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
                    results = query_document_chunks(
                        md5_hash=document_md5,
                        query_embedding=query_embedding,
                        n_results=2
                    )

                    if results['documents'] and results['documents'][0]:
                        document_sources = "\n\nã€æ•™æåŽŸæ–‡å‚è€ƒã€‘\n"
                        for i, doc in enumerate(results['documents'][0][:2], 1):
                            # èŽ·å–é¡µç ä¿¡æ¯
                            page_num = results['metadatas'][0][i].get('page', '?')
                            content = doc[:150] + "..." if len(doc) > 150 else doc
                            document_sources += f"ðŸ“– ç¬¬{page_num}é¡µï¼š{content}\n"
            except Exception as e:
                print(f"æ£€ç´¢æ–‡æ¡£æ¥æºå¤±è´¥: {e}")

        # Get level-specific tutor prompt
        tutor_prompt = get_tutor_prompt(student_level)
        level_info = get_level_description(student_level)

        # Build context from conversation and mistakes
        conversation_context = ""
        if state.get("conversation_history"):
            recent_messages = state["conversation_history"][-3:]
            conversation_context = "\n\nã€æœ€è¿‘å¯¹è¯ã€‘\n"
            for msg in recent_messages:
                if isinstance(msg, HumanMessage):
                    conversation_context += f"å­¦ç”Ÿï¼š{msg.content}\n"
                elif isinstance(msg, AIMessage):
                    conversation_context += f"è€å¸ˆï¼š{msg.content}\n"

        # Address weak points
        review_points = ""
        if wrong_questions:
            review_points = "\n\nã€éœ€è¦é‡ç‚¹å…³æ³¨çš„çŸ¥è¯†ç‚¹ã€‘\n"
            for wq in wrong_questions[-2:]:
                review_points += f"- {wq.get('question', '')}\n"

        system_prompt = f"""{tutor_prompt}

å½“å‰ä¿¡æ¯ï¼š
- å­¦ç”Ÿç­‰çº§ï¼š{level_info['name']}ï¼ˆ{level_info['characteristics']}ï¼‰
- æ•™å­¦é£Žæ ¼ï¼š{level_info['teaching_style']}

æ³¨æ„äº‹é¡¹ï¼š
1. ä¸¥æ ¼æŒ‰ç…§å­¦ç”Ÿç­‰çº§çš„è¯­è¨€é£Žæ ¼å’Œè®²è§£æ–¹å¼
2. é¿å…ä½¿ç”¨è¶…å‡ºç­‰çº§ç†è§£èŒƒå›´çš„ä¸“ä¸šæœ¯è¯­
3. å¤šç”¨é€‚åˆç­‰çº§çš„ä¾‹å­å’Œç±»æ¯”
4. å¯¹äºŽä½Žç­‰çº§ï¼ˆL1-L2ï¼‰ï¼Œè¦æ›´è€å¿ƒã€é¼“åŠ±å¼
5. å¯¹äºŽé«˜ç­‰çº§ï¼ˆL4-L5ï¼‰ï¼Œè¦æ›´æ€è¾¨ã€å¯å‘å¼
6. æŽ§åˆ¶è¾“å‡ºé•¿åº¦ï¼Œç¡®ä¿ä¿¡æ¯å¯†åº¦é€‚ä¸­"""

        # Determine what to explain
        if topic:
            user_prompt = f"è¯·è¯¦ç»†è®²è§£ï¼š{topic}\n"
        elif context:
            user_prompt = f"å­¦ç”Ÿé—®é¢˜ï¼š{context}\n"
        else:
            user_prompt = f"è¯·ä¸ºæœ¬ç« èŠ‚æä¾›æ ¸å¿ƒçŸ¥è¯†ç‚¹çš„è®²è§£ã€‚\n"

        user_prompt += f"""
ç« èŠ‚ï¼š{chapter_title}

å­¦ä¹ ç›®æ ‡ï¼š
{chr(10).join(f'- {obj}' for obj in learning_objectives) if learning_objectives else '- ç†è§£æ ¸å¿ƒæ¦‚å¿µ'}

ç›¸å…³å†…å®¹ï¼š
{chapter_content[:3000]}{review_points}{conversation_context}

è¯·æä¾›é€‚åˆè¯¥ç­‰çº§çš„è®²è§£ã€‚"""

        try:
            response = await self.llm.ainvoke([
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ])

            return response.content

        except Exception as e:
            # Fallback explanation
            return f"""ã€æ ¸å¿ƒçŸ¥è¯†ç‚¹è®²è§£ã€‘

{topic or chapter_title}

å¾ˆæŠ±æ­‰ï¼ŒAI è®²è§£æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚è¯·ç¨åŽå†è¯•ã€‚

æ‚¨å¯ä»¥ï¼š
1. é˜…è¯»ç« èŠ‚å†…å®¹è¿›è¡Œè‡ªå­¦
2. å°è¯•å®Œæˆç»ƒä¹ é¢˜
3. å‘åŠ©æ•™æé—®

é”™è¯¯ä¿¡æ¯ï¼š{str(e)}"""

    async def answer_question(
        self,
        state: TeachingState,
        question: str
    ) -> str:
        """
        Answer a student's question with level-adapted response.

        Args:
            state: Current teaching state
            question: Student's question

        Returns:
            Formatted answer
        """
        student_level = state["student_level"]
        chapter_content = state["chapter_content"]

        # Get context about what student is struggling with
        wrong_topics = [
            wq.get("question", "")[:50]
            for wq in state.get("wrong_questions", [])
        ]

        system_prompt = get_tutor_prompt(student_level)

        user_prompt = f"""å­¦ç”Ÿæé—®ï¼š{question}

å½“å‰ç« èŠ‚èƒŒæ™¯ï¼ˆç”¨äºŽå›žç­”ï¼‰ï¼š
{chapter_content[:2000]}

å­¦ç”Ÿè–„å¼±ç‚¹ï¼š
{chr(10).join(f'- {t}' for t in wrong_topics) if wrong_topics else 'æš‚æ— æ˜Žæ˜¾è–„å¼±ç‚¹'}

è¯·æä¾›é€‚åˆè¯¥ç­‰çº§çš„è§£ç­”ã€‚"""

        try:
            response = await self.llm.ainvoke([
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ])

            return response.content

        except Exception as e:
            return f"æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›žç­”è¿™ä¸ªé—®é¢˜ã€‚è¯·ç¨åŽå†è¯•ã€‚\n\né”™è¯¯ï¼š{str(e)}"

    async def provide_hint(
        self,
        state: TeachingState,
        question: Dict[str, Any],
        attempt_number: int = 1
    ) -> str:
        """
        Provide a progressive hint for a question.

        Args:
            state: Current teaching state
            question: The question the student is struggling with
            attempt_number: Which attempt this is (1st, 2nd, 3rd)

        Returns:
            Hint text
        """
        student_level = state["student_level"]
        question_text = question.get("question", "")
        correct_answer = question.get("correct_answer", "")
        explanation = question.get("explanation", "")

        # Progressive hints based on attempt
        hint_templates = {
            1: {
                1: "ðŸ’¡ æç¤ºï¼šä»”ç»†é˜…è¯»é¢˜ç›®ï¼Œå…³é”®ä¿¡æ¯å°±åœ¨é¢˜å¹²ä¸­ã€‚",
                2: "ðŸ’¡ æç¤º2ï¼šå›žå¿†ä¸€ä¸‹æœ¬ç« èŠ‚çš„æ ¸å¿ƒæ¦‚å¿µã€‚",
                3: "ðŸ’¡ æç¤º3ï¼šè¿™é“é¢˜è€ƒæŸ¥çš„æ˜¯åŸºç¡€çŸ¥è¯†ç‚¹ï¼Œå†æƒ³æƒ³ï¼Ÿ"
            },
            2: {
                1: "ðŸ’¡ æç¤ºï¼šå…ˆç†æ¸…é¢˜ç›®åœ¨é—®ä»€ä¹ˆï¼Œå†æ€è€ƒç›¸å…³çŸ¥è¯†ã€‚",
                2: "ðŸ’¡ æç¤º2ï¼šå¯ä»¥ç”¨æŽ’é™¤æ³•ï¼Œå…ˆæŽ’é™¤æ˜Žæ˜¾é”™è¯¯çš„é€‰é¡¹ã€‚",
                3: "ðŸ’¡ æç¤º3ï¼šå›žåˆ°æ•™æï¼Œæ‰¾åˆ°ç›¸å…³çš„å®šä¹‰æˆ–ä¾‹å­ã€‚"
            },
            3: {
                1: "ðŸ’¡ æç¤ºï¼šåˆ†æžé¢˜ç›®æ¶‰åŠçš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€ä¹ˆã€‚",
                2: "ðŸ’¡ æç¤º2ï¼šè€ƒè™‘ä¸€ä¸‹è¿™ä¸ªæ¦‚å¿µçš„åº”ç”¨åœºæ™¯ã€‚",
                3: "ðŸ’¡ æç¤º3ï¼šå¯¹æ¯”å„ä¸ªé€‰é¡¹ï¼Œæ‰¾å‡ºæœ€ç¬¦åˆå®šä¹‰çš„ç­”æ¡ˆã€‚"
            }
        }

        # For higher levels, give less direct hints
        if student_level >= 4:
            return {
                1: "ðŸ’¡ æç¤ºï¼šæ€è€ƒè¿™ä¸ªé—®é¢˜çš„æœ¬è´¨æ˜¯ä»€ä¹ˆã€‚",
                2: "ðŸ’¡ æç¤º2ï¼šè€ƒè™‘è¾¹ç•Œæ¡ä»¶ã€‚",
                3: "ðŸ’¡ æç¤º3ï¼šä»ŽåŽŸç†å‡ºå‘åˆ†æžã€‚"
            }.get(attempt_number, "ðŸ’¡ å†æ·±å…¥æ€è€ƒä¸€ä¸‹ã€‚")

        hint = hint_templates.get(student_level, hint_templates[3])
        return hint.get(attempt_number, "ðŸ’¡ ç»§ç»­åŠªåŠ›ï¼Œä½ ç¦»ç­”æ¡ˆå¾ˆè¿‘äº†ã€‚")

    async def generate_summary(
        self,
        state: TeachingState
    ) -> str:
        """
        Generate a learning summary based on session performance.

        Args:
            state: Current teaching state with performance data

        Returns:
            Formatted summary with recommendations
        """
        student_level = state["student_level"]
        success_rate = state.get("success_rate", 0.0)
        correct_count = len(state.get("correct_questions", []))
        wrong_count = len(state.get("wrong_questions", []))

        # Build performance summary
        performance_summary = f"""
å­¦ä¹ æƒ…å†µæ€»ç»“

âœ… æ­£ç¡®ï¼š{correct_count} é¢˜
âŒ é”™è¯¯ï¼š{wrong_count} é¢˜
ðŸ“Š æ­£ç¡®çŽ‡ï¼š{success_rate * 100:.1f}%
"""

        if success_rate >= 0.8:
            feedback = "å¤ªæ£’äº†ï¼ä½ å¯¹æœ¬ç« èŠ‚çš„æŽŒæ¡éžå¸¸å¥½ã€‚ç»§ç»­ä¿æŒï¼Œå¯ä»¥å°è¯•æ›´éš¾çš„æŒ‘æˆ˜ã€‚"
        elif success_rate >= 0.6:
            feedback = "ä¸é”™çš„è¡¨çŽ°ï¼å¤§éƒ¨åˆ†æ¦‚å¿µéƒ½å·²ç»ç†è§£ã€‚å»ºè®®é‡ç‚¹å¤ä¹ ä¸€ä¸‹é”™é¢˜æ¶‰åŠçš„çŸ¥è¯†ç‚¹ã€‚"
        else:
            feedback = "å»ºè®®é‡æ–°é˜…è¯»ç« èŠ‚å†…å®¹ï¼Œå·©å›ºåŸºç¡€æ¦‚å¿µã€‚ä¸è¦ç€æ€¥ï¼Œæ‰“å¥½åŸºç¡€å¾ˆé‡è¦ã€‚"

        # Add review recommendations
        review_section = ""
        if wrong_count > 0:
            review_section = "\n\nã€å»ºè®®å¤ä¹ çš„çŸ¥è¯†ç‚¹ã€‘\n"
            for i, wq in enumerate(state["wrong_questions"][:3], 1):
                review_section += f"{i}. {wq.get('question', '')[:50]}...\n"

        return performance_summary + feedback + review_section


# LangGraph node function
async def tutor_node(state: TeachingState) -> TeachingState:
    """
    LangGraph node for the Tutor agent.

    This node provides explanations based on student level.
    """
    # Initialize Tutor agent with DashScope
    model_name = get_model_name(state["student_level"])
    tutor = TutorAgent(model=model_name)

    # Get the last user message if available
    user_question = None
    if state.get("conversation_history"):
        last_msg = state["conversation_history"][-1]
        if isinstance(last_msg, HumanMessage):
            user_question = last_msg.content

    # Generate explanation
    if user_question:
        explanation = await tutor.answer_question(state, user_question)
    else:
        explanation = await tutor.explain_concept(state)

    # Update state
    state["tutor_explanation"] = explanation
    state["streaming_content"] = explanation
    state["current_step"] = "explanation_provided"

    # Add to conversation history
    state["conversation_history"].append(AIMessage(content=explanation))

    return state


async def tutor_hint_node(state: TeachingState, question_id: str, attempt: int) -> TeachingState:
    """
    LangGraph node for providing hints.

    Args:
        state: Current teaching state
        question_id: ID of question needing hint
        attempt: Attempt number (1, 2, 3)

    Returns:
        Updated state with hint
    """
    tutor = TutorAgent()

    # Find the question
    question = next(
        (q for q in state.get("examiner_questions", [])
         if q.get("question_id") == question_id),
        None
    )

    if not question:
        return state

    # Generate hint
    hint = await tutor.provide_hint(state, question, attempt)

    state["streaming_content"] = hint
    state["conversation_history"].append(AIMessage(content=hint))

    return state


async def tutor_summary_node(state: TeachingState) -> TeachingState:
    """
    LangGraph node for generating session summary.

    Args:
        state: Current teaching state with performance data

    Returns:
        Updated state with summary
    """
    tutor = TutorAgent()
    summary = await tutor.generate_summary(state)

    state["tutor_explanation"] = summary
    state["streaming_content"] = summary
    state["feedback"] = summary
    state["current_step"] = "session_complete"

    return state
