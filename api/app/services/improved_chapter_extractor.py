"""
æ”¹è¿›ç‰ˆç« èŠ‚æå–å™¨ - ä¸å¢åŠ æ‰«æèŒƒå›´ï¼Œä½†æ›´èªæ˜åœ°é€‰æ‹©

æ ¸å¿ƒæ”¹è¿›ï¼š
1. ä¿æŒæ‰«æèŒƒå›´ 60 é¡µï¼ˆåˆç†ï¼‰
2. æ™ºèƒ½å®šä½"ç›®å½•"æ ‡é¢˜é¡µ
3. ç¡®ä¿å–åˆ°è¿ç»­çš„å®Œæ•´ç›®å½•ï¼ˆé€šå¸¸ 3-8 é¡µï¼‰
4. LLM æå–åéªŒè¯ï¼Œå¦‚æœå¤ªå°‘åˆ™é‡è¯•
"""
import re
import json
from typing import List, Dict, Any, Optional
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import settings


class ImprovedChapterExtractor:
    """æ”¹è¿›ç‰ˆç« èŠ‚æå–å™¨"""

    def __init__(self):
        self.api_key = settings.DASHSCOPE_API_KEY
        self.model = "qwen-max"
        self.MAX_SCAN_PAGES = 60  # ä¿æŒ 60 é¡µï¼Œåˆç†èŒƒå›´

    async def extract_chapters_from_text(
        self,
        toc_text: str,
        document_id: int,
        user_id: int,
        db: AsyncSession
    ) -> List[Dict[str, Any]]:
        """
        ç›´æ¥ä»ç›®å½•æ–‡æœ¬æå–ç« èŠ‚ï¼ˆä¸éœ€è¦é¡µé¢æ£€æµ‹ï¼‰

        é€‚ç”¨åœºæ™¯ï¼š
        - å·²ç»æœ‰å®Œæ•´çš„ç›®å½•æ–‡æœ¬ï¼ˆå¦‚ä»PDFä¹¦ç­¾æå–ï¼‰
        - ä¸éœ€è¦OCRå¤„ç†çš„PDF

        Args:
            toc_text: å®Œæ•´çš„ç›®å½•æ–‡æœ¬
            document_id: æ–‡æ¡£ID
            user_id: ç”¨æˆ·ID
            db: æ•°æ®åº“ä¼šè¯

        Returns:
            ç« èŠ‚åˆ—è¡¨
        """
        if not toc_text or len(toc_text) < 100:
            print(f"âš ï¸  ç›®å½•æ–‡æœ¬å¤ªçŸ­ï¼ˆ{len(toc_text)}å­—ç¬¦ï¼‰ï¼Œæ— æ³•æå–ç« èŠ‚")
            return []

        print(f"\n{'='*60}")
        print(f"ğŸ“š ç›´æ¥ä»ç›®å½•æ–‡æœ¬æå–ç« èŠ‚")
        print(f"{'='*60}\n")
        print(f"ğŸ“ ç›®å½•æ–‡æœ¬é•¿åº¦: {len(toc_text)} å­—ç¬¦")

        # ç›´æ¥è°ƒç”¨ LLM æå–
        chapters = await self._llm_extract(toc_text, document_id, user_id, db)

        if chapters:
            print(f"\n   âœ… æå–äº† {len(chapters)} ä¸ªç« èŠ‚")
            self._print_chapters(chapters)
            return chapters
        else:
            print(f"\nâŒ LLM æœªèƒ½æå–ç« èŠ‚")
            return []

    async def extract_chapters(
        self,
        ocr_result: Dict[str, Any],
        file_path: str,
        document_id: int,
        user_id: int,
        db: AsyncSession
    ) -> List[Dict[str, Any]]:
        """
        æ”¹è¿›ç‰ˆç« èŠ‚æå–

        ç­–ç•¥ï¼š
        1. æ‰¾åˆ°"ç›®å½•"æ ‡é¢˜é¡µ
        2. ä»ç›®å½•é¡µå¼€å§‹ï¼Œè¿ç»­å–é¡µ
        3. LLM æå–
        4. éªŒè¯ç« èŠ‚æ•°é‡ï¼Œå¤ªå°‘åˆ™é‡è¯•
        """

        print(f"\n{'='*60}")
        print(f"ğŸ“š æ”¹è¿›ç‰ˆç« èŠ‚æå–")
        print(f"{'='*60}\n")

        if not ocr_result or not ocr_result.get('pages'):
            print(f"âŒ æ²¡æœ‰ OCR æ•°æ®")
            return []

        pages = ocr_result['pages']
        print(f"ğŸ“„ å¯ç”¨é¡µé¢ï¼š{len(pages)} é¡µ")

        # ========== ç¬¬ä¸€æ­¥ï¼šæ‰¾åˆ°ç›®å½•æ ‡é¢˜é¡µ ==========
        print(f"\nğŸ” ç¬¬ä¸€æ­¥ï¼šå¯»æ‰¾ç›®å½•æ ‡é¢˜é¡µ...")
        toc_start_page = self._find_toc_title_page(pages)

        if toc_start_page is None:
            print(f"   âš ï¸  æœªæ‰¾åˆ°æ˜ç¡®çš„ç›®å½•æ ‡é¢˜ï¼Œä½¿ç”¨è¯„åˆ†æ–¹æ³•")
            # Fallbackï¼šä½¿ç”¨è¯„åˆ†æ–¹æ³•
            toc_pages = self._select_best_pages_by_score(pages, max_pages=8)
        else:
            print(f"   âœ… æ‰¾åˆ°ç›®å½•æ ‡é¢˜ï¼šç¬¬ {toc_start_page} é¡µ")
            # ========== ç¬¬äºŒæ­¥ï¼šä»ç›®å½•é¡µè¿ç»­æå– ==========
            print(f"\nğŸ“– ç¬¬äºŒæ­¥ï¼šä»ç›®å½•é¡µè¿ç»­æå–...")
            toc_pages = self._extract_continuous_toc_pages(pages, toc_start_page)

            # ğŸ”§ FIX: å¦‚æœé¡µé¢å¤ªå°‘æˆ–æ–‡æœ¬å¤ªçŸ­ï¼Œæ‰©å¤§èŒƒå›´
            if len(toc_pages) < 3 or len(self._merge_toc_pages(toc_pages)) < 2000:
                print(f"   âš ï¸  ç›®å½•é¡µå¤ªå°‘ï¼ˆ{len(toc_pages)}ï¼‰æˆ–æ–‡æœ¬å¤ªçŸ­ï¼Œæ‰©å¤§èŒƒå›´...")
                toc_pages = self._expand_toc_pages(pages, toc_start_page, max_pages=15)
                print(f"   âœ… æ‰©å¤§åï¼š{len(toc_pages)} ä¸ªç›®å½•é¡µ")

        print(f"   âœ… é€‰æ‹©äº† {len(toc_pages)} ä¸ªç›®å½•é¡µï¼š{[p['page_num'] for p in toc_pages]}")

        # ========== ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶æ–‡æœ¬ ==========
        toc_text = self._merge_toc_pages(toc_pages)
        print(f"\nğŸ“ ç¬¬ä¸‰æ­¥ï¼šç›®å½•æ–‡æœ¬é•¿åº¦ {len(toc_text)} å­—ç¬¦")

        # ========== ç¬¬å››æ­¥ï¼šLLM æå– ==========
        print(f"\nğŸ§  ç¬¬å››æ­¥ï¼šLLM æå–ç« èŠ‚...")
        chapters = await self._llm_extract(toc_text, document_id, user_id, db)

        if chapters:
            print(f"\n   âœ… æå–äº† {len(chapters)} ä¸ªç« èŠ‚")

            # ========== ç¬¬äº”æ­¥ï¼šéªŒè¯å’Œé‡è¯• ==========
            if len(chapters) < 3:
                print(f"\nâš ï¸  ç« èŠ‚å¤ªå°‘ï¼ˆ{len(chapters)}ï¼‰ï¼Œå°è¯•æ‰©å¤§èŒƒå›´...")
                return await self._retry_with_more_pages(pages, document_id, user_id, db)

            self._print_chapters(chapters)
            return chapters
        else:
            print(f"\nâŒ LLM æœªèƒ½æå–ç« èŠ‚")
            return []

    def _find_toc_title_page(self, pages: List[Dict]) -> Optional[int]:
        """
        æ‰¾åˆ°"ç›®å½•"æ ‡é¢˜æ‰€åœ¨çš„é¡µé¢

        åŒ¹é…æ¨¡å¼ï¼š
        - "ç›®å½•"ï¼ˆå•ç‹¬æˆè¡Œï¼‰
        - "Contents"ï¼ˆå•ç‹¬æˆè¡Œï¼‰
        - "TABLE OF CONTENTS"
        - "ç›®ã€€å½•"ï¼ˆå¯èƒ½æœ‰ç©ºæ ¼ï¼‰
        """

        toc_patterns = [
            r'^ç›®å½•\s*$',  # "ç›®å½•" ç‹¬ç«‹æˆè¡Œ
            r'^ç›®\s*å½•\s*$',  # "ç›®ã€€å½•"
            r'^Contents\s*$',
            r'^TABLE OF CONTENTS\s*$',
            r'^ç« èŠ‚ç›®å½•\s*$',
        ]

        for page in pages:
            text = page.get('text', '')
            page_num = page.get('page_num')

            # æ£€æŸ¥å‰ 800 å­—ç¬¦ï¼ˆé€šå¸¸æ ‡é¢˜åœ¨å‰é¢ï¼‰
            header = text[:800].strip()

            for pattern in toc_patterns:
                if re.search(pattern, header, re.MULTILINE | re.IGNORECASE):
                    print(f"      â†’ ç¬¬ {page_num} é¡µåŒ¹é…: {pattern[:20]}")
                    return page_num

        return None

    def _extract_continuous_toc_pages(
        self,
        pages: List[Dict],
        start_page: int
    ) -> List[Dict]:
        """
        ä»ç›®å½•æ ‡é¢˜é¡µå¼€å§‹ï¼Œè¿ç»­æå–ç›®å½•é¡µ

        ç­–ç•¥ï¼š
        1. ä»ç›®å½•æ ‡é¢˜é¡µå¼€å§‹
        2. **ç›®å½•æ ‡é¢˜é¡µæœ¬èº«è‡ªåŠ¨åŒ…å«**ï¼ˆå³ä½¿å†…å®¹åªæ˜¯"ç›®å½•"ä¸¤ä¸ªå­—ï¼‰
        3. ç»§ç»­å–é¡µï¼Œç›´åˆ°è¿ç»­ 3 é¡µä¸åƒç›®å½•
        4. æœ€å¤šå– 15 é¡µï¼ˆé˜²æ­¢è¿‡é•¿ï¼‰
        """

        # æ‰¾åˆ°èµ·å§‹é¡µç´¢å¼•
        start_idx = None
        for i, page in enumerate(pages):
            if page['page_num'] == start_page:
                start_idx = i
                break

        if start_idx is None:
            return []

        toc_pages = []
        consecutive_non_toc = 0

        # ğŸ”§ FIX: ä»èµ·å§‹é¡µå¼€å§‹ï¼Œç›®å½•æ ‡é¢˜é¡µæœ¬èº«è‡ªåŠ¨åŒ…å«
        for i in range(start_idx, min(start_idx + 15, len(pages))):
            page = pages[i]
            text = page.get('text', '')

            # ğŸ”§ FIX: ç¬¬ä¸€é¡µï¼ˆç›®å½•æ ‡é¢˜é¡µï¼‰è‡ªåŠ¨åŒ…å«
            is_first_page = (i == start_idx)

            if is_first_page or self._is_likely_toc_page(text):
                toc_pages.append(page)
                consecutive_non_toc = 0
            else:
                consecutive_non_toc += 1
                # æ›´å®½å®¹çš„è¿ç»­æ€§æ£€æµ‹ï¼Œå…è®¸ 3 é¡µä¸åƒç›®å½•æ‰åœæ­¢
                if consecutive_non_toc >= 3:
                    break

        return toc_pages

    def _expand_toc_pages(
        self,
        pages: List[Dict],
        start_page: int,
        max_pages: int = 15
    ) -> List[Dict]:
        """æ‰©å¤§ç›®å½•é¡µé€‰æ‹©èŒƒå›´"""

        # æ‰¾åˆ°èµ·å§‹é¡µç´¢å¼•
        start_idx = None
        for i, page in enumerate(pages):
            if page['page_num'] == start_page:
                start_idx = i
                break

        if start_idx is None:
            return []

        # ä»èµ·å§‹é¡µå¼€å§‹ï¼Œå– max_pages é¡µï¼ˆä¸ç®¡æ˜¯å¦åƒç›®å½•ï¼‰
        # ç›®å½•å¯èƒ½æœ‰ç‰¹æ®Šæ ¼å¼ï¼Œä¸ä¸€å®šæ˜¯å…¸å‹çš„ç« èŠ‚é¡µ
        end_idx = min(start_idx + max_pages, len(pages))

        return pages[start_idx:end_idx]

    def _is_likely_toc_page(self, text: str) -> bool:
        """
        åˆ¤æ–­è¿™é¡µæ˜¯å¦åƒç›®å½•é¡µ

        ç‰¹å¾ï¼š
        1. æœ‰ç« èŠ‚ç¼–å·ï¼ˆ"ç¬¬Xç« "ã€"Chapter X"ï¼‰
        2. æœ‰é¡µç ï¼ˆ"15é¡µ"ã€"P.15"ï¼‰
        3. æœ‰å°èŠ‚ç¼–å·ï¼ˆ"1.1"ã€"1.2"ï¼‰
        4. ç« èŠ‚å…³é”®è¯å¯†åº¦é«˜
        """

        # å¿«é€Ÿæ£€æŸ¥ï¼šå¿…é¡»æœ‰ç« èŠ‚å…³é”®è¯
        has_chapter = (
            'ç« ' in text or 'Chapter' in text or
            re.search(r'\d+\.\d+', text)  # 1.1, 1.2
        )

        if not has_chapter:
            return False

        # è®¡ç®—ç›®å½•ç‰¹å¾åˆ†æ•°
        score = 0

        # ç« èŠ‚ç¼–å·å¯†åº¦
        chapter_matches = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« |Chapter\s+\d+', text)
        if len(chapter_matches) >= 3:
            score += 3
        elif len(chapter_matches) >= 1:
            score += 1

        # å°èŠ‚ç¼–å·å¯†åº¦
        subsection_matches = re.findall(r'\d+\.\d+', text)
        if len(subsection_matches) >= 5:
            score += 2
        elif len(subsection_matches) >= 2:
            score += 1

        # é¡µç å¯†åº¦
        page_matches = re.findall(r'\d+\s*[é¡µpP]|P\.\s*\d+', text)
        if len(page_matches) >= 5:
            score += 2
        elif len(page_matches) >= 2:
            score += 1

        # ç›®å½•å…³é”®è¯
        if 'ç›®å½•' in text[:200] or 'Contents' in text[:200]:
            score += 3

        return score >= 3

    def _select_best_pages_by_score(
        self,
        pages: List[Dict],
        max_pages: int = 8
    ) -> List[Dict]:
        """
        Fallbackï¼šä½¿ç”¨è¯„åˆ†æ–¹æ³•é€‰æ‹©æœ€å¥½çš„é¡µé¢

        è¯„åˆ†æ ‡å‡†ï¼š
        - ç›®å½•å…³é”®è¯ï¼š+10 åˆ†
        - ç« èŠ‚å…³é”®è¯å¯†åº¦ï¼š+3 åˆ†/ä¸ª
        - é¡µç å¯†åº¦ï¼š+2 åˆ†
        """

        scored_pages = []

        for page in pages:
            text = page.get('text', '')
            score = 0

            # ç›®å½•å…³é”®è¯
            toc_keywords = ['ç›®å½•', 'Contents', 'ç›®ã€€å½•', 'TABLE OF CONTENTS']
            for keyword in toc_keywords:
                if keyword in text:
                    score += 10
                    # ç‹¬ç«‹æˆè¡Œé¢å¤–åŠ åˆ†
                    if text.strip().startswith(keyword):
                        score += 5

            # ç« èŠ‚ç¼–å·
            chapter_count = len(re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« |Chapter\s+\d+', text))
            score += chapter_count * 3

            # å°èŠ‚ç¼–å·
            subsection_count = len(re.findall(r'\d+\.\d+', text))
            score += subsection_count * 1

            # é¡µç 
            page_count = len(re.findall(r'\d+\s*[é¡µpP]|P\.\s*\d+', text))
            if page_count > 0:
                density = page_count / len(text) * 1000
                score += min(int(density), 10)

            if score > 0:
                scored_pages.append({
                    'page': page,
                    'score': score
                })

        # æŒ‰åˆ†æ•°æ’åº
        scored_pages.sort(key=lambda x: x['score'], reverse=True)

        # å–å‰ N é¡µ
        best_pages = scored_pages[:max_pages]

        return [p['page'] for p in best_pages]

    def _merge_toc_pages(self, pages: List[Dict]) -> str:
        """åˆå¹¶ç›®å½•é¡µæ–‡æœ¬"""
        parts = []
        for page in pages:
            page_num = page.get('page_num')
            text = page.get('text', '')
            parts.append(f"--- ç¬¬ {page_num} é¡µ ---\n{text}")

        return "\n\n".join(parts)

    async def _llm_extract(
        self,
        toc_text: str,
        document_id: int,
        user_id: int,
        db: AsyncSession
    ) -> Optional[List[Dict[str, Any]]]:
        """ä½¿ç”¨ LLM æå–ç« èŠ‚"""

        print(f"   ğŸ“¤ å‘é€æ–‡æœ¬ç»™ LLMï¼ˆ{len(toc_text)} å­—ç¬¦ï¼‰...")

        # æˆªæ–­æ£€æµ‹ï¼šå¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œåˆ†æ‰¹å¤„ç†
        if len(toc_text) > 6000:
            print(f"   âš ï¸  æ–‡æœ¬è¾ƒé•¿ï¼ˆ{len(toc_text)} å­—ç¬¦ï¼‰ï¼Œå¯èƒ½è¢«æˆªæ–­")
            # è€ƒè™‘åˆ†æ‰¹å¤„ç†æˆ–ä½¿ç”¨æ›´é•¿çš„ä¸Šä¸‹æ–‡æ¨¡å‹

        import httpx

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•™æç›®å½•è¯†åˆ«ä¸“å®¶ã€‚è¯·ä»ä¸‹é¢çš„æ–‡æœ¬ä¸­æå–æ•™æçš„ç›®å½•ç»“æ„ã€‚

ã€å…³é”®è¦æ±‚ã€‘ï¼š
1. **å¿…é¡»æå–æ‰€æœ‰ç« èŠ‚**ï¼Œä¸è¦å› ä¸ºç« èŠ‚å¤šå°±åœæ­¢
2. **å¿…é¡»æå–æ‰€æœ‰å°èŠ‚å’Œå­å°èŠ‚**ï¼ˆ1.1ã€1.1.1 ç­‰ï¼‰
3. å¦‚æœç›®å½•åœ¨å¤šä¸ªé¡µé¢ï¼Œè¦å…¨éƒ¨æå–
4. æ³¨æ„ï¼šä¸è¦æ¼æ‰ä¸­é—´çš„ä»»ä½•ç« èŠ‚

ã€è¾“å‡ºæ ¼å¼ã€‘ï¼ˆä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼ï¼‰ï¼š
{{
  "has_toc": true,
  "confidence": "high",
  "total_chapters": ç« èŠ‚æ€»æ•°ï¼ˆæ•´æ•°ï¼‰,
  "chapters": [
    {{
      "chapter_number": 1,
      "chapter_title": "ç« èŠ‚æ ‡é¢˜ï¼ˆä¸åŒ…å«"ç¬¬Xç« "ï¼‰",
      "page_number": èµ·å§‹é¡µç ,
      "subsections": [
        {{"subsection_number": "1.1", "subsection_title": "å°èŠ‚æ ‡é¢˜", "page_number": é¡µç }},
        {{"subsection_number": "1.2", "subsection_title": "å°èŠ‚æ ‡é¢˜", "page_number": é¡µç }}
      ]
    }}
  ]
}}

ã€ç›®å½•æ–‡æœ¬ã€‘ï¼š
```
{toc_text[:6000]}
```

ã€æ³¨æ„ã€‘ï¼š
- å¦‚æœä¸Šé¢è¢«æˆªæ–­äº†ï¼ˆæ–‡æœ¬åœ¨ç« èŠ‚ä¸­é—´æ–­å¼€ï¼‰ï¼Œå›å¤ "TRUNCATED"ï¼Œæˆ‘ä¼šæä¾›å®Œæ•´æ–‡æœ¬
- å¿…é¡»æå–å®Œæ•´ï¼Œä¸è¦é—æ¼ä»»ä½•ç« èŠ‚
"""

        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„æ•™æç›®å½•è¯†åˆ«ä¸“å®¶ã€‚"},
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": 0.0,  # é™ä½æ¸©åº¦ï¼Œæé«˜ä¸€è‡´æ€§
                        "max_tokens": 8000
                    }
                )

            if response.status_code != 200:
                print(f"   âŒ LLM è°ƒç”¨å¤±è´¥ï¼š{response.status_code}")
                return None

            result = response.json()
            content = result['choices'][0]['message']['content']

            print(f"   ğŸ“¥ LLM è¿”å›å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"   ğŸ“¥ LLM è¿”å›å†…å®¹é¢„è§ˆ:\n{content[:1500]}...\n")

            # æ£€æŸ¥æ˜¯å¦è¢«æˆªæ–­
            if "TRUNCATED" in content or "truncated" in content:
                print(f"   âš ï¸  LLM æŠ¥å‘Šæ–‡æœ¬è¢«æˆªæ–­ï¼Œè‡ªåŠ¨é‡è¯•...")
                # è‡ªåŠ¨ä½¿ç”¨é‡è¯•æœºåˆ¶
                return None  # è®©ä¸Šå±‚è°ƒç”¨ _retry_with_more_pages

            # ğŸ”§ FIX: æ”¹è¿› JSON è§£æ - å°è¯•å¤šç§æ¨¡å¼
            json_str = None

            # æ¨¡å¼1: æ ‡å‡† markdown JSON ä»£ç å—
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', content)
            if json_match:
                json_str = json_match.group(1)
                print(f"   âœ… ä½¿ç”¨æ¨¡å¼1åŒ¹é…ï¼šæ ‡å‡† markdown JSON")
            else:
                # æ¨¡å¼2: ç®€å•ä»£ç å—
                json_match = re.search(r'```\s*([\s\S]*?)\s*```', content)
                if json_match:
                    json_str = json_match.group(1)
                    print(f"   âœ… ä½¿ç”¨æ¨¡å¼2åŒ¹é…ï¼šç®€å•ä»£ç å—")
                else:
                    # æ¨¡å¼3: ç›´æ¥æŸ¥æ‰¾ JSON å¯¹è±¡ï¼ˆä»å¤´åˆ°å°¾ï¼‰
                    json_match = re.search(r'\{[\s\S]*\}', content)
                    if json_match:
                        json_str = json_match.group()
                        print(f"   âœ… ä½¿ç”¨æ¨¡å¼3åŒ¹é…ï¼šçº¯ JSON å¯¹è±¡")
                    else:
                        print(f"   âŒ æ‰€æœ‰ JSON åŒ¹é…æ¨¡å¼éƒ½å¤±è´¥")
                        print(f"   ğŸ“„ åŸå§‹å†…å®¹:\n{content[:500]}...")
                        return None

            if not json_str:
                print(f"   âŒ æœªèƒ½æå– JSON å­—ç¬¦ä¸²")
                return None

            print(f"   ğŸ” æå–çš„ JSON é•¿åº¦: {len(json_str)} å­—ç¬¦")

            # è§£æ JSON
            try:
                data = json.loads(json_str)
                print(f"   âœ… JSON è§£ææˆåŠŸ")
            except json.JSONDecodeError as e:
                print(f"   âŒ JSON è§£æå¤±è´¥: {e}")
                print(f"   ğŸ“„ JSON å­—ç¬¦ä¸²:\n{json_str[:1000]}...")
                # å°è¯•ä¿®å¤å¸¸è§çš„ JSON é—®é¢˜
                try:
                    # å°è¯•ç§»é™¤æœ«å°¾çš„é€—å·
                    fixed_json = re.sub(r',\s*([}\]])', r'\1', json_str)
                    data = json.loads(fixed_json)
                    print(f"   âœ… JSON ä¿®å¤åè§£ææˆåŠŸ")
                except:
                    return None

            if not data.get('has_toc'):
                print(f"   âš ï¸  LLM è®¤ä¸ºæ²¡æœ‰ç›®å½• (has_toc = false)")
                return None

            chapters = data.get('chapters', [])
            if not chapters:
                print(f"   âš ï¸  LLM è¿”å›äº†ç©ºç« èŠ‚åˆ—è¡¨")
                return None

            # åˆ›å»ºæ•°æ®åº“è®°å½•
            for chapter_info in chapters:
                await self._create_chapter_progress(
                    db, document_id, user_id, chapter_info
                )

            return chapters

        except json.JSONDecodeError as e:
            print(f"   âŒ JSON è§£æå¤±è´¥: {e}")
            print(f"   ğŸ“„ LLM å“åº”ï¼š{content[:300]}...")
            return None
        except Exception as e:
            print(f"   âŒ LLM æå–å¤±è´¥: {e}")
            return None

    async def _retry_with_more_pages(
        self,
        pages: List[Dict],
        document_id: int,
        user_id: int,
        db: AsyncSession
    ) -> List[Dict[str, Any]]:
        """é‡è¯•ï¼šä½¿ç”¨æ›´å¤šé¡µé¢"""

        print(f"   ğŸ”„ é‡è¯•ï¼šä½¿ç”¨æ‰€æœ‰ {len(pages)} é¡µ")

        # ä½¿ç”¨æ‰€æœ‰é¡µé¢
        toc_text = self._merge_toc_pages(pages)

        chapters = await self._llm_extract(toc_text, document_id, user_id, db)

        if chapters and len(chapters) >= 3:
            print(f"   âœ… é‡è¯•æˆåŠŸï¼šæå–äº† {len(chapters)} ç« ")
            return chapters
        else:
            print(f"   âŒ é‡è¯•å¤±è´¥")
            return []

    async def _create_chapter_progress(
        self,
        db: AsyncSession,
        document_id: int,
        user_id: int,
        chapter_info: Dict[str, Any]
    ):
        """åˆ›å»ºç« èŠ‚è¿›åº¦è®°å½•å’Œå°èŠ‚è®°å½•"""
        from app.models.document import Progress
        from sqlalchemy import select, text

        print(f"\n{'='*60}")
        print(f"ğŸ’¾ åˆ›å»ºç« èŠ‚è¿›åº¦è®°å½• (ImprovedChapterExtractor)")
        print(f"{'='*60}")
        print(f"ğŸ“‹ chapter_info å†…å®¹: {chapter_info}")
        print(f"ğŸ“‹ chapter_number: {chapter_info.get('chapter_number', 'N/A')}")
        print(f"ğŸ“‹ chapter_title: {chapter_info.get('chapter_title', 'N/A')}")
        print(f"{'='*60}\n")

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = await db.execute(
            select(Progress).where(
                Progress.document_id == document_id,
                Progress.chapter_number == chapter_info.get('chapter_number', 1)
            )
        )
        if existing.scalar_one_or_none():
            print(f"   â„¹ï¸  ç« èŠ‚ {chapter_info.get('chapter_number')} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
            return

        progress = Progress(
            document_id=document_id,
            user_id=user_id,
            chapter_number=chapter_info.get('chapter_number', 1),
            chapter_title=chapter_info.get('chapter_title', 'æœªå‘½åç« èŠ‚'),
            completion_percentage=0.0,
            time_spent_minutes=0,
            is_locked=True
        )

        db.add(progress)
        await db.commit()

        print(f"âœ… ç« èŠ‚è®°å½•åˆ›å»ºæˆåŠŸ:")
        print(f"   - ç« èŠ‚: {progress.chapter_number}")
        print(f"   - æ ‡é¢˜: {progress.chapter_title}")
        print(f"   - ID: {progress.id}")

        # ğŸ”§ NEW: ä¿å­˜å°èŠ‚åˆ°æ•°æ®åº“
        subsections = chapter_info.get('subsections', [])
        if subsections:
            print(f"\nğŸ’¾ ä¿å­˜ {len(subsections)} ä¸ªå°èŠ‚åˆ°æ•°æ®åº“...")

            for subsection_info in subsections:
                try:
                    insert_stmt = text("""
                        INSERT INTO subsections
                        (user_id, document_id, chapter_number, subsection_number,
                         subsection_title, page_number, cognitive_level_assigned,
                         completion_percentage, time_spent_minutes)
                        VALUES (:user_id, :document_id, :chapter_number, :subsection_number,
                                :subsection_title, :page_number, :cognitive_level,
                                :completion_percentage, :time_spent_minutes)
                        ON CONFLICT(user_id, document_id, chapter_number, subsection_number)
                        DO UPDATE SET
                            subsection_title = :subsection_title,
                            page_number = :page_number
                    """)

                    await db.execute(insert_stmt, {
                        "user_id": user_id,
                        "document_id": document_id,
                        "chapter_number": chapter_info.get('chapter_number', 1),
                        "subsection_number": subsection_info.get('subsection_number', ''),
                        "subsection_title": subsection_info.get('subsection_title', ''),
                        "page_number": subsection_info.get('page_number'),
                        "cognitive_level": 3,
                        "completion_percentage": 0.0,
                        "time_spent_minutes": 0.0
                    })

                except Exception as e:
                    print(f"   âš ï¸  å°èŠ‚ {subsection_info.get('subsection_number')} ä¿å­˜å¤±è´¥: {e}")

            await db.commit()
            print(f"âœ… å°èŠ‚ä¿å­˜å®Œæˆ")

    def _print_chapters(self, chapters: List[Dict[str, Any]]):
        """æ‰“å°ç« èŠ‚ä¿¡æ¯"""
        print(f"\n   ğŸ“š æå–çš„ç« èŠ‚ï¼š")
        for ch in chapters:
            subs = ch.get('subsections', [])
            print(f"      ç¬¬ {ch['chapter_number']} ç« ï¼š{ch['chapter_title']}")
            print(f"         å°èŠ‚ï¼š{len(subs)} ä¸ª")
            for sub in subs[:5]:  # åªæ‰“å°å‰ 5 ä¸ªå°èŠ‚
                print(f"           - {sub.get('subsection_number')} {sub.get('subsection_title')}")
            if len(subs) > 5:
                print(f"           ... è¿˜æœ‰ {len(subs) - 5} ä¸ªå°èŠ‚")
