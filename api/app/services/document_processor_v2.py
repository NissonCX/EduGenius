"""
å¢å¼ºç‰ˆæ–‡æ¡£å¤„ç†æœåŠ¡ - æ­£ç¡®å¤„ç†æ•™ç§‘ä¹¦ç›®å½•

ä¸šåŠ¡æµç¨‹ï¼š
1. å•ç‹¬æå–å‰å‡ é¡µï¼ˆå¯èƒ½æ˜¯ç›®å½•é¡µï¼‰
2. è¯†åˆ«çœŸæ­£çš„ç›®å½•é¡µ
3. ä»ç›®å½•é¡µæå–çº¯æ–‡æœ¬ç”¨äºç« èŠ‚åˆ†æ
4. æ­£æ–‡å†…å®¹å•ç‹¬å¤„ç†ï¼ˆä¸åŒ…å«ç›®å½•ï¼‰
"""
import os
import hashlib
import fitz  # PyMuPDF
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document
import dashscope
from dashscope import TextEmbedding

from app.core.config import settings


class EnhancedDocumentProcessor:
    """å¢å¼ºç‰ˆæ–‡æ¡£å¤„ç†å™¨ï¼šæ­£ç¡®å¤„ç†ç›®å½•å’Œæ­£æ–‡"""

    def __init__(self):
        # é…ç½®æ–‡æœ¬åˆ‡åˆ†å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
        )

        # é…ç½® DashScope Embedding
        dashscope.api_key = settings.DASHSCOPE_API_KEY

    def calculate_md5(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œ"""
        md5_hash = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                md5_hash.update(chunk)
        return md5_hash.hexdigest()

    def extract_toc_pages(
        self,
        file_path: str,
        max_toc_pages: int = 15  # å¢åŠ åˆ°15é¡µï¼Œç¡®ä¿åŒ…å«å®Œæ•´ç›®å½•
    ) -> Tuple[List[str], str]:
        """
        æå–å‰å‡ é¡µï¼ˆå¯èƒ½æ˜¯ç›®å½•é¡µï¼‰
        è¿”å›ï¼š(ç›®å½•é¡µåˆ—è¡¨, åˆå¹¶çš„ç›®å½•æ–‡æœ¬)
        """
        print(f"ğŸ“– æ­£åœ¨æå–å‰ {max_toc_pages} é¡µ...")

        try:
            with fitz.open(file_path) as doc:
                toc_pages = []
                toc_text_parts = []
                found_toc_keyword = False
                consecutive_non_toc_pages = 0
                max_consecutive_non_toc = 3  # å…è®¸è¿ç»­3é¡µéç›®å½•ååœæ­¢

                # åªæå–å‰å‡ é¡µ
                for page_num in range(min(max_toc_pages, len(doc))):
                    page = doc[page_num]
                    text = page.get_text().strip()

                    if text:
                        toc_pages.append(text)
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®å½•ç‰¹å¾
                        toc_keywords = ['ç›®å½•', 'ç›®ã€€å½•', 'Contents', 'TABLE OF CONTENTS',
                                        'ç« èŠ‚ç›®å½•', 'CONTENTS', 'è¯¾ã€€é¢˜']
                        has_toc_keyword = any(keyword in text for keyword in toc_keywords)

                        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç« èŠ‚æ ‡è®°ï¼ˆå¦‚"ç¬¬ä¸€ç« "ã€"ç¬¬1ç« "ï¼‰
                        import re
                        has_chapter_marker = re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\s*ç« ', text)

                        # åˆ¤æ–­æ˜¯å¦åº”è¯¥åŒ…å«è¿™ä¸€é¡µ
                        should_include = False

                        if has_toc_keyword:
                            print(f"   âœ… ç¬¬ {page_num + 1} é¡µå‘ç°ç›®å½•å…³é”®è¯")
                            should_include = True
                            found_toc_keyword = True
                            consecutive_non_toc_pages = 0  # é‡ç½®è®¡æ•°å™¨
                        elif has_chapter_marker:
                            # å¦‚æœå‘ç°ç« èŠ‚æ ‡è®°
                            if found_toc_keyword or consecutive_non_toc_pages < max_consecutive_non_toc:
                                print(f"   ğŸ” ç¬¬ {page_num + 1} é¡µå‘ç°ç« èŠ‚æ ‡è®°")
                                should_include = True
                                consecutive_non_toc_pages = 0
                            else:
                                consecutive_non_toc_pages += 1
                        else:
                            # æ™®é€šé¡µé¢
                            if found_toc_keyword and consecutive_non_toc_pages < max_consecutive_non_toc:
                                # åœ¨ç›®å½•åŒºåŸŸåï¼Œå…è®¸ä¸€äº›éç›®å½•é¡µï¼ˆå¦‚ç©ºç™½é¡µã€è¿‡æ¸¡é¡µï¼‰
                                print(f"   ğŸ“„ ç¬¬ {page_num + 1} é¡µ: {len(text)} å­—ç¬¦ï¼ˆç›®å½•åŒºåŸŸï¼‰")
                                should_include = True
                                consecutive_non_toc_pages += 1
                            else:
                                print(f"   ğŸ“„ ç¬¬ {page_num + 1} é¡µ: {len(text)} å­—ç¬¦")
                                consecutive_non_toc_pages += 1

                        # å¦‚æœåº”è¯¥åŒ…å«ï¼Œæ·»åŠ åˆ°ç›®å½•æ–‡æœ¬
                        if should_include:
                            toc_text_parts.append(text)

                        # å¦‚æœè¿ç»­å¤ªå¤šé¡µéƒ½ä¸æ˜¯ç›®å½•ï¼Œåœæ­¢æå–
                        if consecutive_non_toc_pages >= max_consecutive_non_toc and found_toc_keyword:
                            print(f"   â¹ï¸  å·²è¿ç»­{max_consecutive_non_toc}é¡µéç›®å½•ï¼Œåœæ­¢æå–")
                            break

                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„ç›®å½•å…³é”®è¯ï¼Œè¿”å›å‰5é¡µå†…å®¹è®©LLMåˆ¤æ–­
                if not toc_text_parts and toc_pages:
                    print(f"   âš ï¸  æœªå‘ç°æ˜ç¡®çš„ç›®å½•ï¼Œè¿”å›å‰ {min(5, len(toc_pages))} é¡µä¾›åˆ†æ")
                    toc_text_parts = toc_pages[:min(5, len(toc_pages))]
                elif len(toc_text_parts) < 3 and toc_pages:
                    # å¦‚æœæ‰¾åˆ°çš„ç›®å½•å†…å®¹å¤ªå°‘ï¼Œè¡¥å……æ›´å¤šé¡µé¢
                    additional_pages = min(8, len(toc_pages))  # å¢åŠ åˆ°8é¡µ
                    print(f"   ğŸ“ ç›®å½•å†…å®¹è¾ƒå°‘ï¼Œè¡¥å……åˆ°å‰ {additional_pages} é¡µ")
                    toc_text_parts = toc_pages[:additional_pages]

                # åˆå¹¶ç›®å½•é¡µæ–‡æœ¬
                combined_toc = "\n\n".join(toc_text_parts)
                print(f"ğŸ“š ç›®å½•æå–å®Œæˆ: {len(combined_toc)} å­—ç¬¦ï¼Œå…± {len(toc_text_parts)} é¡µ\n")

                return toc_pages, combined_toc

        except Exception as e:
            print(f"âŒ PDF è¯»å–å¤±è´¥: {str(e)}")
            raise

    def identify_chapter_patterns(
        self,
        toc_text: str
    ) -> Dict[str, Any]:
        """
        ä»ç›®å½•æ–‡æœ¬ä¸­è¯†åˆ«ç« èŠ‚æ¨¡å¼
        """
        print("ğŸ” æ­£åœ¨è¯†åˆ«ç« èŠ‚æ¨¡å¼...")

        import re

        # æ£€æŸ¥ä¸åŒçš„ç›®å½•æ ¼å¼
        patterns_found = {}

        # ä¸­æ–‡æ•™æå¸¸è§æ ¼å¼
        if re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç« ', toc_text):
            patterns_found['format'] = 'ä¸­æ–‡ç« èŠ‚ï¼ˆä¸€äºŒä¸‰ï¼‰'
        elif re.search(r'ç¬¬\d+ç« ', toc_text):
            patterns_found['format'] = 'ä¸­æ–‡ç« èŠ‚ï¼ˆæ•°å­—ï¼‰'
        elif re.search(r'Chapter\s*\d+', toc_text, re.IGNORECASE):
            patterns_found['format'] = 'è‹±æ–‡ç« èŠ‚ï¼ˆChapterï¼‰'
        elif re.search(r'^\d+\.', toc_text, re.MULTILINE):
            patterns_found['format'] = 'æ•°å­—ç‚¹ï¼ˆ1. 2.ï¼‰'

        # ç»Ÿè®¡å¯èƒ½çš„ç« èŠ‚æ•°
        chapter_count = len(re.findall(
            r'(?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç« |ç¬¬\d+ç« |Chapter\s*\d+|^\d+\.)',
            toc_text,
            re.MULTILINE
        ))

        patterns_found['estimated_chapters'] = chapter_count

        print(f"   æ£€æµ‹åˆ°æ ¼å¼: {patterns_found.get('format', 'æœªçŸ¥')}")
        print(f"   ä¼°è®¡ç« èŠ‚æ•°: {chapter_count}")

        return patterns_found

    async def process_pdf_v2(
        self,
        file_path: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç† PDF æ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆï¼‰

        è¿”å›åŒ…å«ï¼š
        - toc_text: çº¯å‡€çš„ç›®å½•æ–‡æœ¬
        - content_texts: æ­£æ–‡å†…å®¹ï¼ˆæŒ‰é¡µï¼‰
        - chunks: åˆ‡åˆ†åçš„æ–‡æ¡£å—
        - embeddings: å‘é‡
        - texts: æ‰€æœ‰æ–‡æœ¬
        - stats: ç»Ÿè®¡ä¿¡æ¯
        """
        print("=" * 60)
        print(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {metadata.get('title', 'Unknown')}")
        print("=" * 60)

        # æ­¥éª¤1: æå–ç›®å½•é¡µ
        # ğŸ”§ FIX: å¢åŠ åˆ°15é¡µï¼Œç¡®ä¿åŒ…å«å®Œæ•´ç›®å½•
        toc_pages, toc_text = self.extract_toc_pages(file_path, max_toc_pages=15)

        # æ­¥éª¤2: è¯†åˆ«ç« èŠ‚æ ¼å¼
        toc_info = self.identify_chapter_patterns(toc_text)

        # æ­¥éª¤3: æå–æ‰€æœ‰é¡µé¢å†…å®¹
        print("ğŸ“– æ­£åœ¨æå–å…¨éƒ¨é¡µé¢å†…å®¹...")
        try:
            with fitz.open(file_path) as doc:
                all_pages = []
                toc_page_nums = set()  # è®°å½•ç›®å½•é¡µçš„é¡µç 

                for page_num in range(len(doc)):
                    page = doc[page_num]
                    text = page.get_text().strip()

                    if text:
                        # æ£€æŸ¥è¿™ä¸€é¡µæ˜¯å¦æ˜¯ç›®å½•é¡µ
                        is_toc = any(keyword in text for keyword in
                                 ['ç›®å½•', 'Contents', 'ç›®ã€€å½•'])

                        if is_toc and page_num < 5:
                            toc_page_nums.add(page_num)
                            print(f"   ğŸ“‹ ç¬¬ {page_num + 1} é¡µæ ‡è®°ä¸ºç›®å½•é¡µ")
                        else:
                            # è¿™æ˜¯æ­£æ–‡å†…å®¹
                            all_pages.append({
                                'page': page_num + 1,
                                'content': text
                            })

                print(f"âœ… æå–å®Œæˆ: {len(all_pages)} é¡µæ­£æ–‡ï¼Œ{len(toc_page_nums)} é¡µç›®å½•")

                # æ­¥éª¤4: åˆå¹¶æ­£æ–‡å†…å®¹ï¼ˆä¸åŒ…å«ç›®å½•é¡µï¼‰
                full_content = "\n\n".join([
                    f"--- ç¬¬{p['page']}é¡µ ---\n{p['content']}"
                    for p in all_pages
                ])

                # æ­¥éª¤5: åˆ›å»º Langchain æ–‡æ¡£
                base_metadata = metadata or {}
                documents = [Document(page_content=full_content, metadata=base_metadata)]

                # æ­¥éª¤6: åˆ‡åˆ†æ–‡æ¡£
                print("âœ‚ï¸  æ­£åœ¨åˆ‡åˆ†æ–‡æ¡£...")
                chunks = self.text_splitter.split_documents(documents)

                # ä¸ºæ¯ä¸ª chunk æ·»åŠ  ID
                for i, chunk in enumerate(chunks):
                    chunk.metadata.update({
                        'chunk_id': i,
                        'total_chunks': len(chunks),
                        'source_file': base_metadata.get('title', 'unknown')
                    })

                print(f"   åˆ‡åˆ†å®Œæˆ: {len(chunks)} ä¸ª chunks")

                # æ­¥éª¤7: ç”Ÿæˆå‘é‡
                print("ğŸ§  æ­£åœ¨ç”Ÿæˆå‘é‡...")
                texts = [chunk.page_content for chunk in chunks]
                embeddings = await self.generate_embeddings(texts)

                # æ­¥éª¤8: ç»Ÿè®¡ä¿¡æ¯
                stats = {
                    'total_pages': len(doc),
                    'toc_pages': len(toc_page_nums),
                    'content_pages': len(all_pages),
                    'total_chunks': len(chunks),
                    'total_chars': sum(len(t) for t in texts)
                }

                return {
                    'toc_text': toc_text,  # çº¯å‡€çš„ç›®å½•æ–‡æœ¬ï¼Œç”¨äºç« èŠ‚åˆ†æ
                    'content_texts': [p['content'] for p in all_pages],  # æ­£æ–‡å†…å®¹
                    'chunks': chunks,
                    'embeddings': embeddings,
                    'texts': texts,
                    'stats': stats,
                    'toc_info': toc_info
                }

        except Exception as e:
            print(f"âŒ PDF å¤„ç†å¤±è´¥: {str(e)}")
            raise

    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        try:
            embeddings = []
            batch_size = 25

            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]

                response = TextEmbedding.call(
                    model='text-embedding-v2',
                    input=batch,
                    text_type='document'
                )

                if response.status_code == 200:
                    for emb in response.output['embeddings']:
                        embeddings.append(emb['embedding'])
                else:
                    raise Exception(f"Embedding API é”™è¯¯: {response.message}")

            return embeddings

        except Exception as e:
            print(f"âŒ å‘é‡åŒ–å¤±è´¥: {str(e)}")
            raise
