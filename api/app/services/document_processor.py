"""
Document Processing Service
å¤„ç† PDF/TXT æ–‡æ¡£è§£æã€åˆ‡åˆ†ã€å‘é‡åŒ–çš„å®Œæ•´æµç¨‹
"""
import os
import hashlib
import fitz  # PyMuPDF
from typing import List, Dict, Any, Optional
from pathlib import Path
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document
import dashscope
from dashscope import TextEmbedding

from app.core.config import settings


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ï¼šè§£æã€åˆ‡åˆ†ã€å‘é‡åŒ–"""

    def __init__(self):
        # é…ç½®æ–‡æœ¬åˆ‡åˆ†å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
        )

        # é…ç½® DashScope Embedding
        dashscope.api_key = settings.DASHSCOPE_API_KEY

    async def process_pdf(
        self,
        file_path: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        å¤„ç† PDF æ–‡ä»¶

        Args:
            file_path: PDF æ–‡ä»¶è·¯å¾„
            metadata: é¢å¤–çš„å…ƒæ•°æ®

        Returns:
            åˆ‡åˆ†åçš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨ PyMuPDF è§£æ PDFï¼Œä½¿ç”¨ with è¯­å¥ç¡®ä¿è‡ªåŠ¨å…³é—­
            with fitz.open(file_path) as doc:
                text_content = []
                total_pages = len(doc)  # ä¿å­˜é¡µé¢æ•°é‡
                empty_pages = 0

                print(f"ğŸ“– å¼€å§‹å¤„ç† PDFï¼Œå…± {total_pages} é¡µ")

                for page_num in range(total_pages):
                    try:
                        page = doc[page_num]
                        text = page.get_text()
                        if text.strip():
                            text_content.append({
                                'page': page_num + 1,
                                'content': text
                            })
                        else:
                            empty_pages += 1
                    except Exception as e:
                        # è·³è¿‡æœ‰é—®é¢˜çš„é¡µé¢ï¼Œç»§ç»­å¤„ç†å…¶ä»–é¡µé¢
                        print(f"âš ï¸  è·³è¿‡ç¬¬ {page_num + 1} é¡µï¼Œè§£æé”™è¯¯: {str(e)}")
                        continue

                # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯æ‰«æç‰ˆPDF
                if empty_pages > total_pages * 0.5:
                    print(f"âš ï¸  è­¦å‘Š: {empty_pages}/{total_pages} é¡µæ²¡æœ‰æå–åˆ°æ–‡æœ¬")
                    print(f"ğŸ’¡ è¿™å¯èƒ½æ˜¯æ‰«æç‰ˆPDFï¼Œå»ºè®®ä½¿ç”¨æ”¯æŒOCRçš„å·¥å…·å¤„ç†")

                # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼ŒæŠ›å‡ºé”™è¯¯
                if not text_content:
                    raise ValueError("PDF æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯æ‰«æç‰ˆPDFï¼‰")

                print(f"âœ… æˆåŠŸæå– {len(text_content)}/{total_pages} é¡µçš„æ–‡æœ¬")

                # åˆå¹¶æ‰€æœ‰é¡µé¢æ–‡æœ¬
                full_text = "\n\n".join([
                    f"[ç¬¬{item['page']}é¡µ]\n{item['content']}"
                    for item in text_content
                ])

                # åˆ›å»ºå…ƒæ•°æ®
                base_metadata = {
                    'source': file_path,
                    'type': 'pdf',
                    'total_pages': total_pages
                }
                if metadata:
                    base_metadata.update(metadata)

                # åˆ›å»º Document å¯¹è±¡
                document = Document(page_content=full_text, metadata=base_metadata)

                # åˆ‡åˆ†æ–‡æ¡£
                chunks = self.text_splitter.split_documents([document])

                # ä¸ºæ¯ä¸ª chunk æ·»åŠ æ›´å¤šä¿¡æ¯
                for i, chunk in enumerate(chunks):
                    chunk.metadata.update({
                        'chunk_id': i,
                        'total_chunks': len(chunks)
                    })

                print(f"âœ‚ï¸  æ–‡æ¡£åˆ‡åˆ†å®Œæˆ: {len(chunks)} ä¸ª chunks")

                return chunks

        except Exception as e:
            print(f"âŒ PDF å¤„ç†å¤±è´¥: {str(e)}")
            raise

    async def process_txt(
        self,
        file_path: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        å¤„ç† TXT æ–‡ä»¶

        Args:
            file_path: TXT æ–‡ä»¶è·¯å¾„
            metadata: é¢å¤–çš„å…ƒæ•°æ®

        Returns:
            åˆ‡åˆ†åçš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨ LangChain çš„ TextLoader
            loader = TextLoader(file_path, encoding='utf-8')
            documents = loader.load()

            # æ·»åŠ å…ƒæ•°æ®
            base_metadata = {
                'source': file_path,
                'type': 'txt'
            }
            if metadata:
                base_metadata.update(metadata)

            for doc in documents:
                doc.metadata.update(base_metadata)

            # åˆ‡åˆ†æ–‡æ¡£
            chunks = self.text_splitter.split_documents(documents)

            # ä¸ºæ¯ä¸ª chunk æ·»åŠ  ID
            for i, chunk in enumerate(chunks):
                chunk.metadata.update({
                    'chunk_id': i,
                    'total_chunks': len(chunks)
                })

            return chunks

        except Exception as e:
            raise Exception(f"TXT è§£æå¤±è´¥: {str(e)}")

    async def generate_embeddings(
        self,
        texts: List[str]
    ) -> List[List[float]]:
        """
        ä½¿ç”¨ DashScope ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        try:
            # DashScope Text Embedding API
            # ä½¿ç”¨ text-embedding-v2 æ¨¡å‹
            embeddings = []

            # æ‰¹é‡å¤„ç†ï¼ˆæ¯æ¬¡æœ€å¤š 25 ä¸ªæ–‡æœ¬ï¼‰
            batch_size = 25
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]

                response = TextEmbedding.call(
                    model='text-embedding-v2',
                    input=batch,
                    text_type='document'
                )

                if response.status_code == 200:
                    for emb in response.output['embeddings']:
                        embeddings.append(emb['embedding'])
                else:
                    raise Exception(f"Embedding API é”™è¯¯: {response.message}")

            return embeddings

        except Exception as e:
            raise Exception(f"ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥: {str(e)}")

    async def process_document(
        self,
        file_path: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹ï¼šè§£æ -> åˆ‡åˆ† -> å‘é‡åŒ–

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            metadata: é¢å¤–çš„å…ƒæ•°æ®

        Returns:
            å¤„ç†ç»“æœï¼ŒåŒ…å« chunksã€embeddingsã€ç»Ÿè®¡ä¿¡æ¯
        """
        # åˆ¤æ–­æ–‡ä»¶ç±»å‹
        file_ext = Path(file_path).suffix.lower()

        # è§£ææ–‡æ¡£
        if file_ext == '.pdf':
            chunks = await self.process_pdf(file_path, metadata)
        elif file_ext == '.txt':
            chunks = await self.process_txt(file_path, metadata)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")

        # æå–æ–‡æœ¬ç”¨äºå‘é‡åŒ–
        texts = [chunk.page_content for chunk in chunks]

        # ç”ŸæˆåµŒå…¥å‘é‡
        embeddings = await self.generate_embeddings(texts)

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_chunks': len(chunks),
            'total_characters': sum(len(text) for text in texts),
            'avg_chunk_length': sum(len(text) for text in texts) / len(texts) if texts else 0,
            'embedding_dimension': len(embeddings[0]) if embeddings else 0
        }

        return {
            'chunks': chunks,
            'embeddings': embeddings,
            'stats': stats,
            'texts': texts
        }

    @staticmethod
    def calculate_md5(file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()


# å¯¼å‡ºå‡½æ•°
async def process_uploaded_document(
    file_path: str,
    title: str,
    user_email: str
) -> Dict[str, Any]:
    """
    å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        title: æ–‡æ¡£æ ‡é¢˜
        user_email: ç”¨æˆ·é‚®ç®±

    Returns:
        å¤„ç†ç»“æœ
    """
    processor = DocumentProcessor()

    # è®¡ç®— MD5
    md5_hash = processor.calculate_md5(file_path)

    # å‡†å¤‡å…ƒæ•°æ®
    metadata = {
        'title': title,
        'user_email': user_email,
        'md5': md5_hash
    }

    # å¤„ç†æ–‡æ¡£
    result = await processor.process_document(file_path, metadata)

    # æ·»åŠ  MD5 åˆ°ç»“æœ
    result['md5'] = md5_hash

    return result
