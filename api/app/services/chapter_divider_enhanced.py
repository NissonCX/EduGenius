"""
å¢å¼ºç‰ˆç« èŠ‚åˆ’åˆ†æœåŠ¡ - æ­£ç¡®è§£ææ•™ç§‘ä¹¦ç›®å½•

æ”¹è¿›ç‚¹ï¼š
1. æ¥æ”¶çº¯å‡€çš„ç›®å½•æ–‡æœ¬ï¼ˆä¸å«é¡µé¢æ ‡è®°ï¼‰
2. ä½¿ç”¨æ›´ç²¾å‡†çš„ LLM prompt
3. å¤šå±‚æ¬¡éªŒè¯å’Œ fallback æœºåˆ¶
"""
import json
import re
from typing import List, Dict, Any
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.document import Document, Progress
from app.core.config import settings


class EnhancedChapterDivider:
    """å¢å¼ºç‰ˆç« èŠ‚åˆ’åˆ†å™¨"""

    def __init__(self):
        self.api_key = settings.DASHSCOPE_API_KEY
        self.model = "qwen-max"  # ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹

    async def divide_document_into_chapters(
        self,
        document_id: int,
        user_id: int,
        document_text: str,
        db: AsyncSession
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æ–‡æ¡£æ–‡æœ¬åˆ’åˆ†ç« èŠ‚

        Args:
            document_id: æ–‡æ¡£ ID
            user_id: ç”¨æˆ· ID
            document_text: å®Œæ•´æ–‡æ¡£æ–‡æœ¬ï¼ˆåŒ…å«ç›®å½•ï¼‰
            db: æ•°æ®åº“ä¼šè¯

        Returns:
            ç« èŠ‚åˆ—è¡¨
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“š å¼€å§‹å¤„ç†æ–‡æ¡£ {document_id} çš„ç« èŠ‚åˆ’åˆ†")
        print(f"{'='*60}\n")

        # ä½¿ç”¨å¢å¼ºç‰ˆæç¤ºè¯
        result = await self._extract_toc_with_llm(document_text)

        if result.get('has_toc', False):
            chapters = result.get('chapters', [])
            print(f"\nâœ… LLMæˆåŠŸæå– {len(chapters)} ä¸ªç« èŠ‚:")
            for ch in chapters:
                subs = ch.get('subsections', [])
                print(f"   ç¬¬{ch['chapter_number']}ç« : {ch['chapter_title']} ({len(subs)}ä¸ªå°èŠ‚)")
            print()

            # åˆ›å»ºå­¦ä¹ è¿›åº¦è®°å½•
            for chapter_info in chapters:
                await self._create_chapter_progress(
                    db, document_id, user_id, chapter_info
                )

            return chapters
        else:
            print("\nâš ï¸  æœªæ‰¾åˆ°ç›®å½•ï¼Œä½¿ç”¨å¯å‘å¼æ–¹æ³•\n")
            return await self._heuristic_division_with_fallback(
                db, document_id, user_id, document_text
            )

    async def _extract_toc_with_llm(
        self,
        document_text: str
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM æå–ç›®å½•ç»“æ„

        é‡ç‚¹æ”¹è¿›ï¼š
        1. æ›´æ¸…æ™°çš„ prompt
        2. æä¾›æ›´å¤šç¤ºä¾‹
        3. è¦æ±‚ LLM ç¡®è®¤æ˜¯å¦æ‰¾åˆ°ç›®å½•
        """
        # å¢åŠ æå–çš„æ–‡æœ¬é•¿åº¦ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰ç« èŠ‚
        # å¯¹äºç›®å½•è¾ƒé•¿çš„æ•™æï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ–‡æœ¬
        # ğŸ”§ FIX: ä½¿ç”¨å®Œæ•´çš„æ–‡æœ¬è€Œä¸æ˜¯æˆªæ–­åˆ°30000å­—ç¬¦
        toc_section = document_text  # ä½¿ç”¨å®Œæ•´æ–‡æœ¬

        print("ğŸ“ æ­£åœ¨å‘é€ç»™ LLM åˆ†æç›®å½•...")
        print(f"ğŸ“„ åˆ†ææ–‡æœ¬é•¿åº¦: {len(toc_section)} å­—ç¬¦")
        print(f"ğŸ“„ åŸå§‹æ–‡æœ¬é•¿åº¦: {len(document_text)} å­—ç¬¦")
        if len(toc_section) < len(document_text):
            print(f"âš ï¸  æ–‡æœ¬è¢«æˆªæ–­ï¼åªä½¿ç”¨äº† {len(toc_section)}/{len(document_text)} å­—ç¬¦")

        # æ‰“å°æ–‡æœ¬é¢„è§ˆç”¨äºè°ƒè¯•
        print(f"ğŸ“– æ–‡æœ¬é¢„è§ˆ:\n{toc_section[:500]}...")
        print()

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•™æç›®å½•è¯†åˆ«ä¸“å®¶ã€‚è¯·ä»ä¸‹é¢çš„æ–‡æœ¬ä¸­æå–æ•™æçš„ç›®å½•ç»“æ„ã€‚

ã€é‡è¦è¦æ±‚ã€‘ï¼š
1. ä»”ç»†æŸ¥æ‰¾ç›®å½•éƒ¨åˆ†ï¼Œé€šå¸¸åœ¨æ–‡æ¡£å¼€å¤´ï¼Œä¼šæœ‰"ç›®å½•"ã€"Contents"ç­‰æ ‡é¢˜
2. ç›®å½•åŒ…å«ç« èŠ‚ç¼–å·å’Œæ ‡é¢˜ï¼Œå¯èƒ½æœ‰é¡µç 
3. **å…³é”®ï¼šå¿…é¡»æå–æ–‡æœ¬ä¸­å‡ºç°çš„æ‰€æœ‰ç« èŠ‚ï¼Œä¸è¦é—æ¼ä»»ä½•ç« èŠ‚**
4. è¯·æå–æ‰€æœ‰ç« èŠ‚å’Œå°èŠ‚ä¿¡æ¯
5. å¯¹äºä¸­æ–‡æ•™æï¼Œç‰¹åˆ«æ³¨æ„"ç¬¬ä¸€ç« "ã€"ç¬¬äºŒç« "æˆ–"ç¬¬1ç« "ã€"ç¬¬2ç« "è¿™æ ·çš„æ ¼å¼

ã€æ–‡æœ¬å†…å®¹ã€‘ï¼š
```
{toc_section}
```

ã€ç¤ºä¾‹ - å¦‚ä½•æå–ã€‘ï¼š

ç¤ºä¾‹1 - ä¸­æ–‡æ•™æï¼š
è¾“å…¥ï¼š
```
ç›®å½•
ç¬¬ä¸€ç«   ç‰©è´¨åŠå…¶å˜åŒ–
  ç¬¬ä¸€èŠ‚  ç‰©è´¨çš„åˆ†ç±»åŠè½¬åŒ–        2
  ç¬¬äºŒèŠ‚  ç¦»å­ååº”                10
  ç¬¬ä¸‰èŠ‚  æ°§åŒ–è¿˜åŸååº”            18
ç¬¬äºŒç«   æµ·æ°´ä¸­çš„é‡è¦å…ƒç´ 
  ç¬¬ä¸€èŠ‚  é’ åŠå…¶åŒ–åˆç‰©            28
  ç¬¬äºŒèŠ‚  æ°¯åŠå…¶åŒ–åˆç‰©            36
```
è¾“å‡ºï¼š
{{
  "has_toc": true,
  "confidence": "high",
  "total_chapters": 2,
  "chapters": [
    {{
      "chapter_number": 1,
      "chapter_title": "ç‰©è´¨åŠå…¶å˜åŒ–",
      "page_number": 2,
      "subsections": [
        {{"subsection_number": "1.1", "subsection_title": "ç‰©è´¨çš„åˆ†ç±»åŠè½¬åŒ–", "page_number": 2}},
        {{"subsection_number": "1.2", "subsection_title": "ç¦»å­ååº”", "page_number": 10}},
        {{"subsection_number": "1.3", "subsection_title": "æ°§åŒ–è¿˜åŸååº”", "page_number": 18}}
      ]
    }},
    {{
      "chapter_number": 2,
      "chapter_title": "æµ·æ°´ä¸­çš„é‡è¦å…ƒç´ ",
      "page_number": 28,
      "subsections": [
        {{"subsection_number": "2.1", "subsection_title": "é’ åŠå…¶åŒ–åˆç‰©", "page_number": 28}},
        {{"subsection_number": "2.2", "subsection_title": "æ°¯åŠå…¶åŒ–åˆç‰©", "page_number": 36}}
      ]
    }}
  ]
}}

ã€æå–è§„åˆ™ã€‘ï¼š
1. ç« èŠ‚ç¼–å·æ ¼å¼ï¼š
   - ä¸­æ–‡æ•°å­—ï¼šç¬¬ä¸€ç« ã€ç¬¬äºŒç« ã€ç¬¬ä¸‰ç« 
   - é˜¿æ‹‰ä¼¯æ•°å­—ï¼šç¬¬1ç« ã€ç¬¬2ç« 
   - è‹±æ–‡æ ¼å¼ï¼šChapter 1ã€Chapter 2
   - ç‚¹åºå·ï¼š1.ã€2.ã€3.

2. å°èŠ‚ç¼–å·æ ¼å¼ï¼š
   - ç‚¹åºå·ï¼š1.1ã€1.2ã€2.1ã€2.2
   - ä¸­æ–‡ï¼šç¬¬ä¸€èŠ‚ã€ç¬¬äºŒèŠ‚

3. æ¸…ç†æ ‡é¢˜ï¼š
   - ç§»é™¤ç« èŠ‚ç¼–å·ï¼ˆå¦‚"ç¬¬ä¸€ç« "ï¼‰
   - ç§»é™¤é¡µç ï¼ˆå¦‚"    2"æˆ–"P.2"ï¼‰
   - åªä¿ç•™çº¯æ–‡æœ¬æ ‡é¢˜

ã€è¿”å›æ ¼å¼ã€‘ï¼ˆä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
  "has_toc": true æˆ– false,
  "confidence": "high" æˆ– "medium" æˆ– "low",
  "total_chapters": ç« èŠ‚æ€»æ•°ï¼ˆæ•´æ•°ï¼‰,
  "chapters": [
    {{
      "chapter_number": 1,
      "chapter_title": "ç« èŠ‚æ ‡é¢˜ï¼ˆä¸åŒ…å«ç« èŠ‚å·å’Œé¡µç ï¼‰",
      "page_number": é¡µç æ•°å­—,
      "subsections": [
        {{
          "subsection_number": "1.1",
          "subsection_title": "å°èŠ‚æ ‡é¢˜ï¼ˆä¸åŒ…å«å°èŠ‚å·å’Œé¡µç ï¼‰",
          "page_number": é¡µç æ•°å­—
        }}
      ]
    }}
  ]
}}

ã€åˆ¤æ–­æ ‡å‡†ã€‘ï¼š
- å¦‚æœæ‰¾åˆ°æ¸…æ™°çš„ç« èŠ‚æ ‡é¢˜ï¼ˆæœ‰æ˜ç¡®çš„ç« èŠ‚ç¼–å·å’Œæ ‡é¢˜ï¼‰ï¼Œè®¾ç½® has_toc = true
- å¦‚æœå®Œå…¨æ²¡æœ‰ç›®å½•ç»“æ„æˆ–æ— æ³•è¯†åˆ«ç« èŠ‚ï¼Œè®¾ç½® has_toc = false
- confidence è¡¨ç¤ºä½ çš„ç½®ä¿¡åº¦ï¼šhighï¼ˆéå¸¸ç¡®å®šï¼‰ã€mediumï¼ˆåŸºæœ¬ç¡®å®šï¼‰ã€lowï¼ˆä¸å¤ªç¡®å®šï¼‰
"""

        try:
            # ğŸ”§ FIX: æ”¹ç”¨ OpenAI å…¼å®¹çš„ APIï¼ˆæ›´å¯é ï¼‰
            import httpx

            print(f"ğŸ”‘ ä½¿ç”¨æ¨¡å‹: {self.model}")
            print(f"ğŸ“¡ è°ƒç”¨ DashScope OpenAI å…¼å®¹ API...")

            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„æ•™æç›®å½•è¯†åˆ«ä¸“å®¶ã€‚"},
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": 0.0,
                        "max_tokens": 8000
                    }
                )

                print(f"ğŸ“Š APIå“åº”çŠ¶æ€: {response.status_code}")

                if response.status_code != 200:
                    print(f"âŒ LLM API é”™è¯¯: {response.status_code}")
                    print(f"ğŸ“¦ é”™è¯¯è¯¦æƒ…: {response.text[:500]}")
                    return {"has_toc": False}

                result = response.json()
                content = result['choices'][0]['message']['content']

                if not content or not content.strip():
                    print("âš ï¸  LLM è¿”å›äº†ç©ºå“åº”")
                    return {"has_toc": False}

                content = content.strip()
                print(f"ğŸ“¥ LLM è¿”å›å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                print(f"ğŸ“¥ LLM è¿”å›å†…å®¹é¢„è§ˆ:\n{content[:1500]}...\n")

                # ğŸ”§ FIX: æ”¹è¿› JSON è§£æ - å°è¯•å¤šç§æ¨¡å¼
                json_str = None

                # æ¨¡å¼1: æ ‡å‡† markdown JSON ä»£ç å—
                json_match = re.search(r'```json\s*([\s\S]*?)\s*```', content)
                if json_match:
                    json_str = json_match.group(1)
                    print("âœ… ä½¿ç”¨æ¨¡å¼1åŒ¹é…ï¼šæ ‡å‡† markdown JSON")
                else:
                    # æ¨¡å¼2: ç®€å•ä»£ç å—
                    json_match = re.search(r'```\s*([\s\S]*?)\s*```', content)
                    if json_match:
                        json_str = json_match.group(1)
                        print("âœ… ä½¿ç”¨æ¨¡å¼2åŒ¹é…ï¼šç®€å•ä»£ç å—")
                    else:
                        # æ¨¡å¼3: ç›´æ¥æŸ¥æ‰¾ JSON å¯¹è±¡ï¼ˆä»å¤´åˆ°å°¾ï¼‰
                        json_match = re.search(r'\{[\s\S]*\}', content)
                        if json_match:
                            json_str = json_match.group()
                            print("âœ… ä½¿ç”¨æ¨¡å¼3åŒ¹é…ï¼šçº¯ JSON å¯¹è±¡")
                        else:
                            print("âŒ æ‰€æœ‰ JSON åŒ¹é…æ¨¡å¼éƒ½å¤±è´¥")
                            print(f"ğŸ“„ åŸå§‹å†…å®¹:\n{content}")
                            return {"has_toc": False}

                if not json_str:
                    print("âŒ æœªèƒ½æå– JSON å­—ç¬¦ä¸²")
                    return {"has_toc": False}

                print(f"ğŸ” æå–çš„ JSON é•¿åº¦: {len(json_str)} å­—ç¬¦")
                print(f"ğŸ” JSON é¢„è§ˆ:\n{json_str[:500]}...")

                # è§£æ JSON
                try:
                    parsed_result = json.loads(json_str)
                    print("âœ… JSON è§£ææˆåŠŸ")
                except json.JSONDecodeError as e:
                    print(f"âŒ JSON è§£æå¤±è´¥: {e}")
                    print(f"ğŸ“„ JSON å­—ç¬¦ä¸²:\n{json_str[:1000]}...")
                    # å°è¯•ä¿®å¤å¸¸è§çš„ JSON é—®é¢˜
                    try:
                        # å°è¯•ç§»é™¤æœ«å°¾çš„é€—å·
                        fixed_json = re.sub(r',\s*([}\]])', r'\1', json_str)
                        parsed_result = json.loads(fixed_json)
                        print("âœ… JSON ä¿®å¤åè§£ææˆåŠŸ")
                    except:
                        return {"has_toc": False}

                # éªŒè¯ç»“æœ
                if not parsed_result.get('has_toc'):
                    print("âš ï¸  LLM è®¤ä¸ºæ²¡æœ‰ç›®å½• (has_toc = false)")
                    return {"has_toc": False}

                chapters = parsed_result.get('chapters', [])
                if not chapters:
                    print("âš ï¸  LLM è¿”å›äº†ç©ºç« èŠ‚åˆ—è¡¨")
                    return {"has_toc": False}

                print(f"âœ… LLM è¯†åˆ«æˆåŠŸï¼Œå…± {len(chapters)} ç« :")
                for ch in chapters:
                    subs = ch.get('subsections', [])
                    print(f"   ç¬¬{ch.get('chapter_number', 'N/A')}ç« : {ch.get('chapter_title', 'N/A')}")
                    if subs:
                        print(f"      å°èŠ‚æ•°: {len(subs)}")
                        for sub in subs[:3]:
                            print(f"        - {sub.get('subsection_number', '')} {sub.get('subsection_title', '')}")
                        if len(subs) > 3:
                            print(f"        ... å…± {len(subs)} ä¸ªå°èŠ‚")

                return parsed_result

        except httpx.TimeoutException:
            print("âŒ LLM API è°ƒç”¨è¶…æ—¶")
            return {"has_toc": False}
        except httpx.RequestError as e:
            print(f"âŒ LLM API ç½‘ç»œé”™è¯¯: {e}")
            return {"has_toc": False}
        except Exception as e:
            print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {"has_toc": False}

    async def _heuristic_division_with_fallback(
        self,
        db: AsyncSession,
        document_id: int,
        user_id: int,
        document_text: str
    ) -> List[Dict[str, Any]]:
        """å¯å‘å¼ç« èŠ‚åˆ’åˆ†ï¼ˆæ”¹è¿›ç‰ˆ - è¯†åˆ«æ‰€æœ‰ç« èŠ‚ï¼‰"""
        print("ğŸ” ä½¿ç”¨å¯å‘å¼æ–¹æ³•è¯†åˆ«ç« èŠ‚...")

        # ğŸ”§ FIX: ä½¿ç”¨å®Œæ•´æ–‡æœ¬è€Œä¸æ˜¯æˆªæ–­
        search_text = document_text

        print(f"ğŸ“„ æœç´¢æ–‡æœ¬é•¿åº¦: {len(search_text)} å­—ç¬¦")

        # æ”¹è¿›çš„æ­£åˆ™æ¨¡å¼ï¼Œæ›´å‡†ç¡®åŒ¹é…ç« èŠ‚æ ‡é¢˜
        patterns = [
            # ä¸­æ–‡ç« èŠ‚ï¼ˆæœ€å¸¸è§ï¼‰- ç¬¬ä¸€ç«  ç‰©è´¨åŠå…¶å˜åŒ–
            # ä½¿ç”¨éè´ªå©ªåŒ¹é…ï¼Œé¿å…åŒ¹é…è¿‡å¤šå†…å®¹
            {
                'pattern': r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+)\s*ç« [ï¼š:\s]*([^\n]{2,100}?)(?=\nç¬¬|\n\s*ç¬¬|\n\s*\d+\.|\n\n|$)',
                'type': 'chinese_chapter',
                'converter': lambda x: self._chinese_to_number(x)
            },
            # æ•°å­—ç« èŠ‚ - ç¬¬1ç«  ç‰©è´¨åŠå…¶å˜åŒ–
            {
                'pattern': r'ç¬¬(\d+)\s*ç« [ï¼š:\s]*([^\n]{2,100}?)(?=\nç¬¬|\n\s*ç¬¬|\n\s*\d+\.|\n\n|$)',
                'type': 'number_chapter',
                'converter': lambda x: int(x)
            },
            # è‹±æ–‡ç« èŠ‚ - Chapter 1 Matter and Its Changes
            {
                'pattern': r'Chapter\s+(\d+)\s*[:ï¼š]?\s*([^\n]{5,100}?)(?=\s+Chapter|\n\n|$)',
                'type': 'english_chapter',
                'converter': lambda x: int(x)
            },
            # ç‚¹åºå·ç« èŠ‚ - 1. ç‰©è´¨åŠå…¶å˜åŒ–
            {
                'pattern': r'^\s*(\d+)\.\s+([^\n]{5,100}?)(?=\s+\d+\.\s|\n\n|$)',
                'type': 'dot_number',
                'converter': lambda x: int(x)
            },
        ]

        # å°è¯•æ¯ç§æ¨¡å¼
        for pattern_info in patterns:
            pattern = pattern_info['pattern']
            converter = pattern_info['converter']

            matches = list(re.finditer(pattern, search_text, re.MULTILINE))

            if not matches:
                print(f"âš ï¸ '{pattern_info['type']}' æ¨¡å¼æœªæ‰¾åˆ°åŒ¹é…")
                continue

            chapters = []
            seen_titles = set()  # ç”¨äºå»é‡

            for match in matches:
                try:
                    chapter_num = converter(match.group(1))  # æå–ç« èŠ‚å·
                    chapter_title = match.group(2).strip()   # æå–ç« èŠ‚æ ‡é¢˜

                    # æ¸…ç†æ ‡é¢˜ï¼šç§»é™¤é¡µç ã€å¤šä½™ç©ºæ ¼ã€ç‰¹æ®Šå­—ç¬¦
                    chapter_title = re.sub(r'\s*\d+\s*$', '', chapter_title)  # ç§»é™¤æœ«å°¾é¡µç 
                    chapter_title = re.sub(r'\s+', ' ', chapter_title)  # è§„èŒƒç©ºæ ¼
                    chapter_title = chapter_title.strip()

                    # è¿‡æ»¤ï¼šæ ‡é¢˜ä¸èƒ½å¤ªçŸ­ï¼Œä¸”ä¸èƒ½é‡å¤
                    if chapter_title and len(chapter_title) > 2:
                        # ä½¿ç”¨ç« èŠ‚å·å’Œæ ‡é¢˜çš„ç»„åˆä½œä¸ºå”¯ä¸€æ ‡è¯†
                        title_key = f"{chapter_num}_{chapter_title}"
                        if title_key not in seen_titles:
                            seen_titles.add(title_key)
                            chapters.append({
                                'chapter_number': chapter_num,
                                'chapter_title': chapter_title,
                                'page_number': 1  # é»˜è®¤é¡µç 
                            })
                except Exception as e:
                    print(f"   âš ï¸  å¤„ç†åŒ¹é…é¡¹å¤±è´¥: {e}")
                    continue

            if chapters and len(chapters) >= 1:
                print(f"âœ… ä½¿ç”¨ '{pattern_info['type']}' æ¨¡å¼æ‰¾åˆ° {len(chapters)} ç« :")
                for ch in chapters:
                    print(f"   ç¬¬{ch['chapter_number']}ç« : {ch['chapter_title']}")

                # ä¸ºæ¯ä¸ªç« èŠ‚å°è¯•è¯†åˆ«å°èŠ‚
                for chapter_info in chapters:
                    # å°è¯•ä»æœç´¢æ–‡æœ¬ä¸­æå–è¯¥ç« èŠ‚çš„å°èŠ‚
                    subsections = self._extract_subsections_for_chapter(
                        search_text,
                        chapter_info['chapter_number']
                    )

                    if subsections:
                        chapter_info['subsections'] = subsections
                        print(f"      ğŸ’¾ ç¬¬{chapter_info['chapter_number']}ç« è¯†åˆ«åˆ° {len(subsections)} ä¸ªå°èŠ‚")

                    # åˆ›å»ºè¿›åº¦è®°å½•ï¼ˆåŒ…æ‹¬å°èŠ‚ï¼‰
                    await self._create_chapter_progress(
                        db, document_id, user_id, chapter_info
                    )

                return chapters
            else:
                print(f"âš ï¸ '{pattern_info['type']}' æ¨¡å¼æœªæ‰¾åˆ°è¶³å¤Ÿçš„ç« èŠ‚")

        # æœ€åçš„ fallbackï¼šåˆ›å»ºå•ä¸ªé»˜è®¤ç« èŠ‚
        print("âš ï¸  æ— æ³•è¯†åˆ«ç« èŠ‚ï¼Œåˆ›å»ºé»˜è®¤ç« èŠ‚")

        # å°è¯•ä»æ•°æ®åº“è·å–æ–‡æ¡£æ ‡é¢˜
        from app.models.document import Document
        doc_result = await db.execute(
            select(Document).where(Document.id == document_id)
        )
        document = doc_result.scalar_one_or_none()
        doc_title = document.title if document else "å®Œæ•´æ•™æ"

        default_chapter = {
            'chapter_number': 1,
            'chapter_title': doc_title,  # ä½¿ç”¨æ–‡æ¡£æ ‡é¢˜è€Œä¸æ˜¯"é»˜è®¤ç« èŠ‚"
            'page_number': 1
        }
        await self._create_chapter_progress(
            db, document_id, user_id, default_chapter
        )

        return [default_chapter]

    async def _create_chapter_progress(
        self,
        db: AsyncSession,
        document_id: int,
        user_id: int,
        chapter_info: Dict[str, Any]
    ):
        """åˆ›å»ºç« èŠ‚å­¦ä¹ è¿›åº¦è®°å½•ï¼ˆåŒ…æ‹¬å°èŠ‚ï¼‰"""
        try:
            print(f"\n{'='*60}")
            print(f"ğŸ’¾ åˆ›å»ºç« èŠ‚è¿›åº¦è®°å½•")
            print(f"{'='*60}")
            print(f"ğŸ“‹ chapter_info å†…å®¹: {chapter_info}")
            print(f"ğŸ“‹ chapter_number: {chapter_info.get('chapter_number', 'N/A')}")
            print(f"ğŸ“‹ chapter_title: {chapter_info.get('chapter_title', 'N/A')}")
            print(f"ğŸ“‹ page_number: {chapter_info.get('page_number', 'N/A')}")
            print(f"{'='*60}\n")

            # è·å–ç”¨æˆ·çš„è®¤çŸ¥ç­‰çº§
            from app.models.document import User
            from app.models.subsection import Subsection
            user_result = await db.execute(
                select(User).where(User.id == user_id)
            )
            user = user_result.scalar_one_or_none()
            cognitive_level = user.cognitive_level if user else 3

            # åˆ›å»ºç« èŠ‚è¿›åº¦è®°å½•
            progress = Progress(
                user_id=user_id,
                document_id=document_id,
                chapter_number=chapter_info.get('chapter_number', 1),
                chapter_title=chapter_info.get('chapter_title', 'æœªå‘½åç« èŠ‚'),
                cognitive_level_assigned=cognitive_level,
                completion_percentage=0,
                time_spent_minutes=0
            )

            db.add(progress)
            await db.commit()

            print(f"âœ… ç« èŠ‚è®°å½•åˆ›å»ºæˆåŠŸ:")
            print(f"   - ç« èŠ‚: {progress.chapter_number}")
            print(f"   - æ ‡é¢˜: {progress.chapter_title}")
            print(f"   - ID: {progress.id}")

            # å¦‚æœæœ‰å°èŠ‚ä¿¡æ¯ï¼Œä¹Ÿåˆ›å»ºå°èŠ‚è®°å½•
            subsections = chapter_info.get('subsections', [])
            if subsections:
                print(f"\n   ğŸ’¾ å¼€å§‹åˆ›å»º {len(subsections)} ä¸ªå°èŠ‚è®°å½•...")
                from sqlalchemy import text
                success_count = 0
                for idx, subsection_info in enumerate(subsections, 1):
                    try:
                        # ä½¿ç”¨executemanyæ‰¹é‡æ’å…¥ï¼Œé¿å…å‚æ•°æ ¼å¼é—®é¢˜
                        insert_stmt = text("""
                            INSERT INTO subsections
                            (user_id, document_id, chapter_number, subsection_number,
                             subsection_title, page_number, cognitive_level_assigned,
                             completion_percentage, time_spent_minutes)
                            VALUES (:user_id, :document_id, :chapter_number, :subsection_number,
                                    :subsection_title, :page_number, :cognitive_level,
                                    :completion_pct, :time_spent)
                        """)

                        subsection_number = subsection_info.get('subsection_number', '')
                        subsection_title = subsection_info.get('subsection_title', '')

                        await db.execute(insert_stmt, {
                            'user_id': user_id,
                            'document_id': document_id,
                            'chapter_number': chapter_info.get('chapter_number', 1),
                            'subsection_number': subsection_number,
                            'subsection_title': subsection_title,
                            'page_number': subsection_info.get('page_number', 1),
                            'cognitive_level': cognitive_level,
                            'completion_pct': 0.0,
                            'time_spent': 0.0
                        })
                        success_count += 1
                        print(f"      âœ… [{idx}/{len(subsections)}] å°èŠ‚ {subsection_number}: {subsection_title}")
                    except Exception as e:
                        print(f"      âš ï¸  [{idx}/{len(subsections)}] åˆ›å»ºå°èŠ‚å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        continue

                await db.commit()
                print(f"\n   âœ… å°èŠ‚åˆ›å»ºå®Œæˆ: {success_count}/{len(subsections)} æˆåŠŸ")
            else:
                print(f"   â„¹ï¸  æ­¤ç« èŠ‚æ²¡æœ‰å°èŠ‚ä¿¡æ¯")

        except Exception as e:
            print(f"âŒ åˆ›å»ºç« èŠ‚è¿›åº¦å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    def _extract_subsections_for_chapter(
        self,
        search_text: str,
        chapter_number: int
    ) -> List[Dict[str, Any]]:
        """ä¸ºæŒ‡å®šç« èŠ‚æå–å°èŠ‚"""
        subsections = []

        # å®šä¹‰å°èŠ‚åŒ¹é…æ¨¡å¼
        subsection_patterns = [
            # æ•°å­—å°èŠ‚ï¼š1.1ã€1.2 ç­‰
            rf'{chapter_number}\.(\d+)\s*[.ã€:ï¼š]?\s*([^\n]{{2,80}}?)(?=\n{chapter_number}\.|\n\n|\nç¬¬|\Z)',
            # ä¸­æ–‡å°èŠ‚ï¼šç¬¬ä¸€èŠ‚ã€ç¬¬äºŒèŠ‚ç­‰
            rf'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*èŠ‚\s*[.ã€:ï¼š]?\s*([^\n]{{2,80}}?)(?=\nç¬¬|\n\n|\Z)',
        ]

        import re
        for pattern in subsection_patterns:
            matches = re.finditer(pattern, search_text, re.MULTILINE)
            for match in matches:
                try:
                    if chapter_number == int(match.group(1)):  # ç¡®ä¿æ˜¯å½“å‰ç« èŠ‚çš„å°èŠ‚
                        subsection_title = match.group(2).strip()
                        subsection_title = re.sub(r'\s*\d+\s*$', '', subsection_title)  # ç§»é™¤é¡µç 
                        subsection_title = re.sub(r'\s+', ' ', subsection_title)

                        if subsection_title and len(subsection_title) > 2:
                            subsection_num = match.group(1)
                            subsections.append({
                                'subsection_number': f"{chapter_number}.{subsection_num}",
                                'subsection_title': subsection_title,
                                'page_number': 1
                            })
                except (ValueError, IndexError):
                    continue

            if subsections:
                break  # å¦‚æœæ‰¾åˆ°å°èŠ‚ï¼Œåœæ­¢å°è¯•å…¶ä»–æ¨¡å¼

        return subsections

    def _chinese_to_number(self, chinese_num: str) -> int:
        """å°†ä¸­æ–‡æ•°å­—è½¬æ¢ä¸ºé˜¿æ‹‰ä¼¯æ•°å­—"""
        chinese_map = {
            'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
            'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
            'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13, 'åå››': 14, 'åäº”': 15,
            'åå…­': 16, 'åä¸ƒ': 17, 'åå…«': 18, 'åä¹': 19, 'äºŒå': 20
        }

        # å¤„ç†"äºŒåä¸€"åˆ°"ä¹åä¹"
        if chinese_num in chinese_map:
            return chinese_map[chinese_num]

        # ç®€å•å¤„ç†ï¼šæå–æ•°å­—éƒ¨åˆ†
        match = re.search(r'\d+', chinese_num)
        if match:
            return int(match.group(0))

        return 1


# å¯¼å‡ºå‡½æ•°
async def process_document_v2(
    file_path: str,
    title: str,
    user_email: str
) -> Dict[str, Any]:
    """å¤„ç†æ–‡æ¡£ï¼ˆv2ï¼‰"""
    processor = EnhancedDocumentProcessor()

    md5_hash = processor.calculate_md5(file_path)

    metadata = {
        'title': title,
        'user_email': user_email,
        'md5': md5_hash
    }

    # å¤„ç† PDF
    result = await processor.process_pdf_v2(file_path, metadata)

    result['md5'] = md5_hash
    return result
